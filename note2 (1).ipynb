{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni4T_dwEHFVa"
   },
   "source": [
    "## CS 6120: Natural Language Processing - Prof. Ahmad Uzair\n",
    "\n",
    "### Assignment 2: Text Classification and Neural Network\n",
    "### Total Points: 100 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loQ22M-bCubq"
   },
   "source": [
    "In Assignment 2, you will be dealing with text classification using Multinomial Naive Bayes and Neural Networks. You will also be dealing with vector visualization. In the previous assingment you implemented Bag of Words as the feature selection method. However, in this assignment you will be using TF-IDF Vectorization instead of Bag of Words. We recommend starting with this assignment a little early as the datasets are quite large and several parts of the assignment might take long duration to execute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3brglC1C0mZ"
   },
   "source": [
    "## Question 1 Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ1y75IoC3rE"
   },
   "source": [
    "In the first question you will be dealing with 20 News Group Dataset. You are required to implement TF-IDF vectorization from scratch and perform Multinomial Naive Bayes Classification on the News Group Dataset.\n",
    "You may use appropriate packages or modules for fitting the Multinomial Naive Bayes Model, however, the implementation of the TF-IDF Vectorization should be from the scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRjM45PyC8ML"
   },
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "Link to the original dataset: http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
    "\n",
    "You can also import the dataset from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CmFf2INNDJRb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "AEp1SHe5DKCB"
   },
   "outputs": [],
   "source": [
    "# Import the 20 news group dataset utilizing sklearn library\n",
    "#categories = ['alt.atheism']\n",
    "\n",
    "mydata_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "\n",
    "mydata_test = fetch_20newsgroups(subset='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "OscOmcE0DMHa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Print the news groups(target) in the dataset\n",
    "\n",
    "pprint(list(mydata_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "JvQb2r0aDR_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "<class 'sklearn.utils._bunch.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "# What is the type of 'mydata_train' and 'mydata_test'\n",
    "\n",
    "print(type(mydata_train))\n",
    "print(type(mydata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ozzwyREhDaMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "11314\n",
      "7532\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the data\n",
    "\n",
    "print(len(mydata_train.data))\n",
    "print(len(mydata_train.filenames))\n",
    "print(len(mydata_test.data))\n",
    "print(len(mydata_test.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlipMuEpDz-K"
   },
   "source": [
    "### Expected Output: \n",
    "11314\n",
    "\n",
    "11314\n",
    "\n",
    "7532\n",
    "\n",
    "7532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55FKOBBuEDI2"
   },
   "source": [
    "## Extracting Features from the Dataset                        (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4GDENzmEEkG"
   },
   "source": [
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgxfDXmxEHid"
   },
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qNFFYyEKsa"
   },
   "source": [
    "Our model cannot simply read the text data so we convert it into numerical format. In order to convert the data into numerical format we create vectors from text.\n",
    "\n",
    "For this particular purpose we could either employ Bag of Words or TF-IDF Vectorization\n",
    "\n",
    "Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency, which instead of giving more weight to words that occur more frequently, it gives a higher weight to words that occur less frequently.\n",
    "\n",
    "Ref:https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/#:~:text=Bag%20of%20Words%20just%20creates,less%20important%20ones%20as%20well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLzgRJRZEP3k"
   },
   "source": [
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe5lHi3NE1QJ"
   },
   "source": [
    "Term Frequency is the measure of the frequency of words in a document. It is the ratio of the number of times the word appears in a document compared to the total number of words in that document.\n",
    "\n",
    "The words that occur rarely in the corpus have a high IDF score. It is the log of the ratio of the number of documents to the number of documents containing the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXotAER_EQ8T"
   },
   "source": [
    "idf(t) = log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "2vzVI8ylFAEl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = mydata_train.data\n",
    "test = mydata_test.data\n",
    "# for line in text:\n",
    "#     print (line)\n",
    "#     words = line.split()\n",
    "#     print(words)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2AatmvFtE-"
   },
   "source": [
    "## Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used stemming operation for text-preprocessing\n",
    "def clean_review(review):\n",
    "    no_html = BeautifulSoup(review).get_text()\n",
    "    clean = re.sub(\"[^a-z\\s]+\", \" \", no_html, flags=re.IGNORECASE)\n",
    "    clean = re.sub(\"(\\s+)\", \" \", clean)\n",
    "#     clean = clean.lower()\n",
    "#     stopwords_en = stopwords.words(\"english\")\n",
    "#     cleaned_stopwords = [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]\n",
    "    cleaned_stopwords = clean.split()\n",
    "    stemmed_words = []\n",
    "    for w in cleaned_stopwords:\n",
    "        stemmed_words.append(snowball.stem(w))\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "XyJbe42AFuHp"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "lines = [] \n",
    "word_list = [] \n",
    " \n",
    "# for line in text:\n",
    "#     #tokenize the text documents and update the lists word_list and lines  \n",
    "#     each_line_word_list = clean_review(line)\n",
    "#     lines.append(each_line_word_list)\n",
    "# word_list = list(set(chain(*lines)))                \n",
    "    \n",
    "# # Make sure the word_list contains unique tokens\n",
    "\n",
    "\n",
    "# # Calculate the total documents present in the corpus\n",
    "# total_docs = len(mydata_train.data)\n",
    "\n",
    " \n",
    "# dict_idx = {}\n",
    "# for word in word_list:\n",
    "#     index = word_list.index(word)\n",
    "#     dict_idx[word] = index\n",
    "\n",
    "\n",
    "\n",
    "#copied from net\n",
    "\n",
    "for sent in text:\n",
    "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
    "    lines.append(x)\n",
    "    for word in x:\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "            \n",
    "word_list = set(word_list)\n",
    "total_documents = len(lines)\n",
    "\n",
    "dict_idx = {} #Dictionary to store index for each word\n",
    "i = 0\n",
    "for word in word_list:\n",
    "    dict_idx[word] = i\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "E6M_CoI9FxYb"
   },
   "outputs": [],
   "source": [
    "# Create a frequency dictionary\n",
    " \n",
    "def frequency_dict(lines):\n",
    "    '''\n",
    "    lines: list containing all the tokens\n",
    "    ---\n",
    "    freq_word: returns a dictionary which keeps the count of the number of documents containing the given word\n",
    "    '''\n",
    "#     freq_word = {}\n",
    "#     for each_list in lines:\n",
    "#         for token in each_list:\n",
    "#             if freq_word.get(token) == None :\n",
    "#                 freq_word[token] = 1\n",
    "#             else:\n",
    "#                 freq_word[token] = freq_word[token] + 1 \n",
    "#     return freq_word\n",
    "\n",
    "\n",
    "#copied from internet\n",
    "\n",
    "    freq_word = {}\n",
    "    for word in word_list:\n",
    "        freq_word[word] = 0\n",
    "        for line in lines:\n",
    "            if word in line:\n",
    "                freq_word[word] += 1\n",
    "    return freq_word\n",
    " \n",
    "#word_count = count_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "qFkt9KBgFz43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duckworth': 1,\n",
       " 'srscnslt': 1,\n",
       " 'sammons': 9,\n",
       " 'actaual': 1,\n",
       " 'circulation': 10,\n",
       " 'ydvp': 1,\n",
       " 'jab': 1,\n",
       " 'transylvania': 1,\n",
       " 'mwtilden': 1,\n",
       " 'uaccess': 12,\n",
       " 'privelege': 1,\n",
       " 'etherlink': 1,\n",
       " 'lauches': 1,\n",
       " 'servo': 2,\n",
       " 'millenia': 8,\n",
       " 'lim': 15,\n",
       " 'freeport': 2,\n",
       " 'goudswaard': 3,\n",
       " 'reinstate': 4,\n",
       " 'evidentally': 1,\n",
       " 'legged': 1,\n",
       " 'cashed': 1,\n",
       " 'mro': 1,\n",
       " 'unfortunate': 42,\n",
       " 'whitsell': 5,\n",
       " 'extraction': 6,\n",
       " 'dscy': 2,\n",
       " 'retain': 29,\n",
       " 'pencom': 1,\n",
       " 'acceptably': 3,\n",
       " 'averting': 1,\n",
       " 'costello': 6,\n",
       " 'cherbayev': 1,\n",
       " 'zrcimrck': 1,\n",
       " 'preparation': 20,\n",
       " 'modalistic': 1,\n",
       " 'cleaning': 44,\n",
       " 'cjjj': 1,\n",
       " 'ariane': 8,\n",
       " 'portugal': 7,\n",
       " 'floatingpoint': 1,\n",
       " 'howe': 20,\n",
       " 'mpoly': 2,\n",
       " 'quibble': 11,\n",
       " 'luftmeister': 1,\n",
       " 'tennyson': 1,\n",
       " 'instate': 2,\n",
       " 'xws': 1,\n",
       " 'weevils': 2,\n",
       " 'wilhelm': 2,\n",
       " 'xtndepth': 1,\n",
       " 'ridiculed': 1,\n",
       " 'marblehead': 1,\n",
       " 'pleasant': 32,\n",
       " 'thyssen': 1,\n",
       " 'ujc': 1,\n",
       " 'chartered': 2,\n",
       " 'foolkiller': 2,\n",
       " 'mbyets': 1,\n",
       " 'hashemites': 1,\n",
       " 'boone': 6,\n",
       " 'hearts': 44,\n",
       " 'slumping': 2,\n",
       " 'gandalf': 5,\n",
       " 'flamsteed': 1,\n",
       " 'ld': 30,\n",
       " 'omschrijving': 1,\n",
       " 'jaze': 1,\n",
       " 'high': 716,\n",
       " 'pluck': 1,\n",
       " 'nerves': 17,\n",
       " 'irretrievably': 1,\n",
       " 'detritus': 1,\n",
       " 'classically': 2,\n",
       " 'kratz': 34,\n",
       " 'bmwoa': 2,\n",
       " 'breadtubes': 1,\n",
       " 'sharen': 4,\n",
       " 'sinha': 6,\n",
       " 'micali': 2,\n",
       " 'hhva': 1,\n",
       " 'lifespans': 1,\n",
       " 'neurasthenia': 1,\n",
       " 'honourable': 4,\n",
       " 'hundley': 2,\n",
       " 'martyriologia': 1,\n",
       " 'westbrook': 1,\n",
       " 'riddle': 25,\n",
       " 'fabric': 5,\n",
       " 'gizli': 1,\n",
       " 'bounding': 2,\n",
       " 'yesterdaysdate': 1,\n",
       " 'grendel': 1,\n",
       " 'olmus': 1,\n",
       " 'diagnostic': 21,\n",
       " 'learned': 118,\n",
       " 'amadi': 1,\n",
       " 'digitalequipmentcorporation': 1,\n",
       " 'resemble': 11,\n",
       " 'labeled': 23,\n",
       " 'inquisitors': 1,\n",
       " 'devprogram': 1,\n",
       " 'hetersexual': 1,\n",
       " 'stipulations': 2,\n",
       " 'wonderfully': 5,\n",
       " 'jsut': 2,\n",
       " 'stooke': 2,\n",
       " 'decoders': 5,\n",
       " 'depress': 2,\n",
       " 'renewable': 4,\n",
       " 'thinkers': 5,\n",
       " 'xdeleted': 1,\n",
       " 'brader': 11,\n",
       " 'recoup': 3,\n",
       " 'mitten': 1,\n",
       " 'sigterm': 1,\n",
       " 'yellows': 2,\n",
       " 'middlemen': 1,\n",
       " 'lockheed': 23,\n",
       " 'baltic': 4,\n",
       " 'aeons': 1,\n",
       " 'qdvbgtc': 1,\n",
       " 'mantick': 3,\n",
       " 'comison': 1,\n",
       " 'bmj': 1,\n",
       " 'glutamate': 21,\n",
       " 'pean': 1,\n",
       " 'craziest': 1,\n",
       " 'ixqj': 1,\n",
       " 'shitcom': 1,\n",
       " 'msgptr': 1,\n",
       " 'sunglasses': 8,\n",
       " 'overgeneralization': 1,\n",
       " 'stacy': 1,\n",
       " 'ezel': 1,\n",
       " 'schuster': 2,\n",
       " 'academica': 1,\n",
       " 'heddings': 1,\n",
       " 'guessed': 25,\n",
       " 'whittier': 3,\n",
       " 'sepa': 1,\n",
       " 'spaghetti': 2,\n",
       " 'befoe': 1,\n",
       " 'keats': 1,\n",
       " 'lyall': 6,\n",
       " 'virginity': 2,\n",
       " 'oanb': 1,\n",
       " 'thai': 2,\n",
       " 'belting': 1,\n",
       " 'nucelotide': 1,\n",
       " 'upgrade': 126,\n",
       " 'lawbreaker': 1,\n",
       " 'took': 400,\n",
       " 'npvzi': 1,\n",
       " 'mr': 107,\n",
       " 'hourly': 2,\n",
       " 'pc': 372,\n",
       " 'royal': 64,\n",
       " 'tbwjh': 1,\n",
       " 'chuckle': 12,\n",
       " 'sendkeys': 1,\n",
       " 'jml': 1,\n",
       " 'followinf': 1,\n",
       " 'mho': 4,\n",
       " 'psychologically': 4,\n",
       " 'fledgling': 2,\n",
       " 'switz': 2,\n",
       " 'vilayet': 5,\n",
       " 'downbeat': 1,\n",
       " 'xhrbc': 1,\n",
       " 'wasteland': 1,\n",
       " 'mollica': 1,\n",
       " 'suppression': 3,\n",
       " 'petes': 3,\n",
       " 'rcmd': 1,\n",
       " 'scranton': 2,\n",
       " 'souzexpertisa': 1,\n",
       " 'rubeae': 4,\n",
       " 'laud': 2,\n",
       " 'due': 394,\n",
       " 'investements': 1,\n",
       " 'suicidal': 9,\n",
       " 'monopolism': 1,\n",
       " 'pillsbury': 1,\n",
       " 'yek': 1,\n",
       " 'illegitimately': 1,\n",
       " 'actionm': 1,\n",
       " 'exaggerate': 4,\n",
       " 'hyphae': 4,\n",
       " 'dama': 1,\n",
       " 'typos': 10,\n",
       " 'valuemask': 3,\n",
       " 'uninc': 7,\n",
       " 'hemmorrhoid': 1,\n",
       " 'oftentimes': 3,\n",
       " 'baylor': 11,\n",
       " 'thyagi': 11,\n",
       " 'mmec': 1,\n",
       " 'formally': 16,\n",
       " 'wll': 1,\n",
       " 'feedwater': 2,\n",
       " 'kaltiyin': 1,\n",
       " 'gemeinhart': 1,\n",
       " 'gpk': 1,\n",
       " 'essentially': 129,\n",
       " 'kahn': 10,\n",
       " 'neurovascular': 1,\n",
       " 'unneeded': 1,\n",
       " 'mcgrath': 1,\n",
       " 'dissolution': 3,\n",
       " 'thrice': 3,\n",
       " 'mtz': 2,\n",
       " 'pyschiatrist': 1,\n",
       " 'congenially': 6,\n",
       " 'microstation': 1,\n",
       " 'attractiveness': 2,\n",
       " 'creatio': 1,\n",
       " 'enamel': 1,\n",
       " 'ajvc': 1,\n",
       " 'straightens': 3,\n",
       " 'infill': 1,\n",
       " 'lovg': 2,\n",
       " 'ddd': 3,\n",
       " 'translucent': 2,\n",
       " 'centrifuge': 3,\n",
       " 'heartily': 9,\n",
       " 'yazz': 1,\n",
       " 'absorbers': 1,\n",
       " 'suglia': 1,\n",
       " 'replies': 125,\n",
       " 'bunuel': 1,\n",
       " 'hould': 1,\n",
       " 'sugar': 35,\n",
       " 'spgs': 1,\n",
       " 'busting': 8,\n",
       " 'kronk': 1,\n",
       " 'ovy': 1,\n",
       " 'catalina': 4,\n",
       " 'actuation': 1,\n",
       " 'convicingly': 1,\n",
       " 'multiplication': 3,\n",
       " 'hamstring': 1,\n",
       " 'constabulary': 1,\n",
       " 'inevitablity': 1,\n",
       " 'albelin': 3,\n",
       " 'maksut': 1,\n",
       " 'scourging': 3,\n",
       " 'penalizing': 2,\n",
       " 'rocicrusian': 1,\n",
       " 'asbestos': 15,\n",
       " 'seirio': 5,\n",
       " 'rjf': 3,\n",
       " 'lesc': 2,\n",
       " 'mellodew': 1,\n",
       " 'bosse': 1,\n",
       " 'transkei': 1,\n",
       " 'healthy': 65,\n",
       " 'tuinstrd': 6,\n",
       " 'rv': 9,\n",
       " 'krew': 1,\n",
       " 'enlighting': 1,\n",
       " 'tolerates': 1,\n",
       " 'pump': 27,\n",
       " 'unlikely': 111,\n",
       " 'porthole': 1,\n",
       " 'increases': 53,\n",
       " 'captivity': 3,\n",
       " 'talanted': 2,\n",
       " 'liquid': 30,\n",
       " 'attracts': 2,\n",
       " 'kepley': 2,\n",
       " 'sjc': 1,\n",
       " 'amendments': 18,\n",
       " 'summarise': 3,\n",
       " 'germinal': 2,\n",
       " 'xtappsettypeconverter': 4,\n",
       " 'writes': 5751,\n",
       " 'mzurmw': 1,\n",
       " 'lyddy': 4,\n",
       " 'ndallen': 10,\n",
       " 'defaultrootwindow': 7,\n",
       " 'formulate': 8,\n",
       " 'puzzled': 15,\n",
       " 'shortstop': 16,\n",
       " 'submitted': 31,\n",
       " 'socalist': 1,\n",
       " 'bit': 793,\n",
       " 'defecate': 2,\n",
       " 'les': 30,\n",
       " 'knowability': 1,\n",
       " 'raider': 1,\n",
       " 'zebedee': 2,\n",
       " 'mcw': 1,\n",
       " 'asthsma': 1,\n",
       " 'dregistere': 1,\n",
       " 'monocrome': 1,\n",
       " 'integrals': 1,\n",
       " 'mustard': 6,\n",
       " 'lngut': 1,\n",
       " 'enthusiasts': 10,\n",
       " 'jarryl': 1,\n",
       " 'spacify': 4,\n",
       " 'wjqj': 1,\n",
       " 'hockey': 233,\n",
       " 'shapewindow': 1,\n",
       " 'noao': 2,\n",
       " 'electrophoresis': 4,\n",
       " 'sq': 19,\n",
       " 'reducible': 3,\n",
       " 'facs': 3,\n",
       " 'reformation': 4,\n",
       " 'palace': 12,\n",
       " 'conically': 1,\n",
       " 'khudurova': 1,\n",
       " 'merton': 3,\n",
       " 'seymo': 1,\n",
       " 'mmw': 2,\n",
       " 'savannah': 1,\n",
       " 'hambidge': 19,\n",
       " 'capus': 2,\n",
       " 'uterine': 1,\n",
       " 'wadddoes': 1,\n",
       " 'segmentation': 9,\n",
       " 'zaandam': 3,\n",
       " 'cristiano': 1,\n",
       " 'guide': 114,\n",
       " 'bkline': 2,\n",
       " 'vlbzrl': 1,\n",
       " 'logistical': 4,\n",
       " 'plase': 3,\n",
       " 'fnajr': 2,\n",
       " 'neath': 2,\n",
       " 'arky': 2,\n",
       " 'biol': 1,\n",
       " 'ukrphil': 1,\n",
       " 'statistisk': 2,\n",
       " 'anasaz': 1,\n",
       " 'afairs': 9,\n",
       " 'proceedeth': 1,\n",
       " 'talionis': 1,\n",
       " 'vulnerabilites': 1,\n",
       " 'siyasi': 2,\n",
       " 'zju': 1,\n",
       " 'minh': 3,\n",
       " 'incorrect': 71,\n",
       " 'hoffa': 2,\n",
       " 'moins': 10,\n",
       " 'oxidises': 1,\n",
       " 'fins': 1,\n",
       " 'cooperstown': 1,\n",
       " 'vacaville': 2,\n",
       " 'arrgggghhhh': 1,\n",
       " 'usian': 1,\n",
       " 'caprette': 1,\n",
       " 'interestend': 1,\n",
       " 'jesper': 3,\n",
       " 'nationalities': 5,\n",
       " 'applies': 97,\n",
       " 'sometimes': 342,\n",
       " 'substantial': 49,\n",
       " 'star': 118,\n",
       " 'sppace': 1,\n",
       " 'cleared': 24,\n",
       " 'pct': 10,\n",
       " 'zur': 3,\n",
       " 'mlap': 1,\n",
       " 'louise': 1,\n",
       " 'rigorous': 18,\n",
       " 'resul': 1,\n",
       " 'decant': 3,\n",
       " 'aud': 2,\n",
       " 'fore': 5,\n",
       " 'urbane': 1,\n",
       " 'perenial': 1,\n",
       " 'corelmark': 4,\n",
       " 'xtgrabnone': 1,\n",
       " 'sigelman': 1,\n",
       " 'tvinfo': 1,\n",
       " 'fokes': 11,\n",
       " 'marquees': 1,\n",
       " 'suggeted': 1,\n",
       " 'bweulv': 1,\n",
       " 'agabus': 1,\n",
       " 'stretch': 38,\n",
       " 'brazenio': 1,\n",
       " 'entertain': 11,\n",
       " 'favorites': 8,\n",
       " 'azzam': 1,\n",
       " 'sethf': 1,\n",
       " 'contorted': 2,\n",
       " 'nobodies': 1,\n",
       " 'notdaw': 1,\n",
       " 'rpms': 6,\n",
       " 'tzxb': 1,\n",
       " 'cccccc': 1,\n",
       " 'eskulap': 1,\n",
       " 'mattix': 1,\n",
       " 'llcoolj': 1,\n",
       " 'grim': 5,\n",
       " 'parroted': 3,\n",
       " 'forteresse': 1,\n",
       " 'kitaguchi': 2,\n",
       " 'juha': 7,\n",
       " 'huh': 92,\n",
       " 'mom': 34,\n",
       " 'arromdee': 29,\n",
       " 'rebuttals': 1,\n",
       " 'qhs': 1,\n",
       " 'checklist': 2,\n",
       " 'inboxes': 2,\n",
       " 'kasler': 1,\n",
       " 'gentile': 20,\n",
       " 'xsizehints': 4,\n",
       " 'pagel': 1,\n",
       " 'stardog': 2,\n",
       " 'bonds': 36,\n",
       " 'repentance': 20,\n",
       " 'desqviewx': 2,\n",
       " 'bushes': 3,\n",
       " 'urbin': 2,\n",
       " 'tend': 155,\n",
       " 'judizers': 1,\n",
       " 'festival': 9,\n",
       " 'recruitment': 6,\n",
       " 'jamma': 1,\n",
       " 'aziz': 5,\n",
       " 'paschich': 1,\n",
       " 'complemented': 1,\n",
       " 'fumbled': 1,\n",
       " 'dobie': 1,\n",
       " 'nead': 4,\n",
       " 'bumping': 3,\n",
       " 'thought': 811,\n",
       " 'censured': 2,\n",
       " 'soennichsen': 1,\n",
       " 'fouling': 1,\n",
       " 'whw': 1,\n",
       " 'disqualification': 1,\n",
       " 'connectivity': 7,\n",
       " 'luna': 6,\n",
       " 'leftout': 1,\n",
       " 'gland': 4,\n",
       " 'wre': 1,\n",
       " 'offerred': 1,\n",
       " 'navigate': 2,\n",
       " 'harass': 7,\n",
       " 'starting': 206,\n",
       " 'consultant': 43,\n",
       " 'republicans': 37,\n",
       " 'amassed': 4,\n",
       " 'hiroaki': 1,\n",
       " 'congressgrades': 1,\n",
       " 'cognac': 1,\n",
       " 'koinoia': 1,\n",
       " 'erzeroum': 1,\n",
       " 'uab': 5,\n",
       " 'objecten': 1,\n",
       " 'mills': 3,\n",
       " 'pfz': 1,\n",
       " 'mcrosbie': 1,\n",
       " 'nervousness': 2,\n",
       " 'xhh': 1,\n",
       " 'dent': 13,\n",
       " 'stiffer': 4,\n",
       " 'kizlarini': 1,\n",
       " 'asap': 11,\n",
       " 'famous': 83,\n",
       " 'satellite': 74,\n",
       " 'initialy': 1,\n",
       " 'measurex': 1,\n",
       " 'krh': 2,\n",
       " 'lprsn': 1,\n",
       " 'primitives': 6,\n",
       " 'pursuasive': 2,\n",
       " 'ref': 36,\n",
       " 'harangues': 1,\n",
       " 'char': 26,\n",
       " 'bang': 33,\n",
       " 'melanie': 1,\n",
       " 'klossner': 5,\n",
       " 'castor': 1,\n",
       " 'ivker': 4,\n",
       " 'anthropomorphise': 1,\n",
       " 'flinstone': 1,\n",
       " 'kjrlkir': 1,\n",
       " 'angola': 5,\n",
       " 'palisades': 2,\n",
       " 'gamers': 2,\n",
       " 'scharfy': 25,\n",
       " 'pwatkins': 1,\n",
       " 'annexes': 1,\n",
       " 'mailinglist': 2,\n",
       " 'sophamore': 1,\n",
       " 'gepeinigt': 1,\n",
       " 'sunt': 1,\n",
       " 'genrad': 1,\n",
       " 'freiberg': 1,\n",
       " 'imbeded': 1,\n",
       " 'dr': 65,\n",
       " 'drye': 1,\n",
       " 'exegetical': 2,\n",
       " 'pkane': 1,\n",
       " 'zelots': 1,\n",
       " 'umanitoba': 4,\n",
       " 'acp': 1,\n",
       " 'celery': 1,\n",
       " 'maltais': 4,\n",
       " 'markey': 1,\n",
       " 'sidewall': 1,\n",
       " 'mactools': 1,\n",
       " 'limelight': 1,\n",
       " 'xbell': 2,\n",
       " 'respiratory': 6,\n",
       " 'wgp': 2,\n",
       " 'olin': 1,\n",
       " 'praktische': 2,\n",
       " 'wurst': 1,\n",
       " 'eccentric': 8,\n",
       " 'verging': 1,\n",
       " 'rescue': 17,\n",
       " 'brynas': 3,\n",
       " 'motorola': 137,\n",
       " 'wght': 1,\n",
       " 'ldh': 1,\n",
       " 'wanton': 2,\n",
       " 'sommerfeld': 2,\n",
       " 'komite': 2,\n",
       " 'thant': 3,\n",
       " 'officialese': 2,\n",
       " 'jw': 13,\n",
       " 'biegel': 1,\n",
       " 'mapped': 25,\n",
       " 'uncertainty': 10,\n",
       " 'cement': 3,\n",
       " 'tunk': 1,\n",
       " 'lah': 4,\n",
       " 'diddleysquatpoop': 1,\n",
       " 'rr': 17,\n",
       " 'logicon': 2,\n",
       " 'eqj': 1,\n",
       " 'hfe': 2,\n",
       " 'rossiya': 1,\n",
       " 'uev': 2,\n",
       " 'republished': 1,\n",
       " 'xtwindowofobject': 6,\n",
       " 'means': 683,\n",
       " 'accompanying': 19,\n",
       " 'soyuz': 6,\n",
       " 'thedialog': 1,\n",
       " 'evokes': 2,\n",
       " 'westboro': 9,\n",
       " 'slater': 2,\n",
       " 'dimple': 1,\n",
       " 'snivelling': 1,\n",
       " 'prutchi': 6,\n",
       " 'forsaking': 4,\n",
       " 'governed': 11,\n",
       " 'buf': 24,\n",
       " 'predictably': 3,\n",
       " 'encryp': 2,\n",
       " 'internationalized': 1,\n",
       " 'fairwell': 1,\n",
       " 'matrices': 4,\n",
       " 'bitblaster': 1,\n",
       " 'banker': 2,\n",
       " 'jxi': 1,\n",
       " 'ogil': 1,\n",
       " 'atul': 2,\n",
       " 'weeny': 1,\n",
       " 'witha': 3,\n",
       " 'bloodcount': 3,\n",
       " 'respectful': 5,\n",
       " 'combinations': 22,\n",
       " 'consensually': 3,\n",
       " 'tvc': 1,\n",
       " 'falcione': 4,\n",
       " 'villanova': 4,\n",
       " 'capriccioso': 9,\n",
       " 'vznl': 1,\n",
       " 'antagonize': 1,\n",
       " 'proposals': 35,\n",
       " 'immaturity': 6,\n",
       " 'vangen': 1,\n",
       " 'maritime': 1,\n",
       " 'diagrams': 18,\n",
       " 'denial': 34,\n",
       " 'whl': 1,\n",
       " 'telescoping': 3,\n",
       " 'allmartin': 2,\n",
       " 'coccidiodes': 1,\n",
       " 'slams': 3,\n",
       " 'bloodstreams': 1,\n",
       " 'action': 279,\n",
       " 'subwoofer': 4,\n",
       " 'mcreynolds': 3,\n",
       " 'nehls': 1,\n",
       " 'kch': 2,\n",
       " 'works': 595,\n",
       " 'apm': 1,\n",
       " 'quietely': 1,\n",
       " 'herman': 9,\n",
       " 'tns': 1,\n",
       " 'cuny': 1,\n",
       " 'tegretol': 1,\n",
       " 'earlobe': 4,\n",
       " 'ozvk': 1,\n",
       " 'propositions': 3,\n",
       " 'vyf': 2,\n",
       " 'maginal': 1,\n",
       " 'understandings': 1,\n",
       " 'snapix': 1,\n",
       " 'remappings': 2,\n",
       " 'contenintal': 1,\n",
       " 'finkelstein': 1,\n",
       " 'excesses': 5,\n",
       " 'murderous': 11,\n",
       " 'bull': 45,\n",
       " 'rlhz': 2,\n",
       " 'fro': 2,\n",
       " 'basa': 2,\n",
       " 'mih': 1,\n",
       " 'consume': 23,\n",
       " 'kinikliouglu': 1,\n",
       " 'hillman': 1,\n",
       " 'truckers': 2,\n",
       " 'inadequate': 20,\n",
       " 'expresses': 14,\n",
       " 'tcc': 1,\n",
       " 'alishaw': 1,\n",
       " 'ifthe': 1,\n",
       " 'hesitation': 2,\n",
       " 'characteristically': 2,\n",
       " 'bram': 3,\n",
       " 'bmw': 126,\n",
       " 'decoding': 15,\n",
       " 'spindle': 1,\n",
       " 'subcontinent': 1,\n",
       " 'sgp': 1,\n",
       " 'entried': 1,\n",
       " 'drv': 1,\n",
       " 'nesim': 1,\n",
       " 'zware': 1,\n",
       " 'mobasseri': 5,\n",
       " 'scandanavians': 1,\n",
       " 'hangs': 36,\n",
       " 'staticcolor': 2,\n",
       " 'wechselbaelge': 1,\n",
       " 'europeenne': 1,\n",
       " 'caruth': 1,\n",
       " 'jaehyung': 2,\n",
       " 'alfred': 9,\n",
       " 'jagani': 3,\n",
       " 'gomorrha': 1,\n",
       " 'ces': 2,\n",
       " 'dfeldman': 2,\n",
       " 'ahaz': 1,\n",
       " 'reserving': 1,\n",
       " 'portruding': 1,\n",
       " 'digit': 20,\n",
       " 'keysyms': 3,\n",
       " 'waldrop': 2,\n",
       " 'ruzun': 1,\n",
       " 'tried': 499,\n",
       " 'eorsat': 1,\n",
       " 'intelligences': 2,\n",
       " 'ermeni': 7,\n",
       " 'supremisists': 1,\n",
       " 'restricting': 22,\n",
       " 'odyssey': 3,\n",
       " 'cryptologists': 2,\n",
       " 'challe': 1,\n",
       " 'specificity': 2,\n",
       " 'vakiflar': 1,\n",
       " 'ingest': 2,\n",
       " 'serveral': 2,\n",
       " 'michaelr': 1,\n",
       " 'crnlthry': 1,\n",
       " 'boadilla': 1,\n",
       " 'ethical': 29,\n",
       " 'fractally': 2,\n",
       " 'fairground': 1,\n",
       " 'sexists': 1,\n",
       " 'ajv': 1,\n",
       " 'gamete': 1,\n",
       " 'chastisement': 3,\n",
       " 'capricornus': 2,\n",
       " 'dof': 3,\n",
       " 'unplug': 11,\n",
       " 'cyc': 1,\n",
       " 'doit': 5,\n",
       " 'ahl': 27,\n",
       " 'intellectually': 15,\n",
       " 'luser': 3,\n",
       " 'resolder': 1,\n",
       " 'decorational': 1,\n",
       " 'veff': 1,\n",
       " 'cfv': 7,\n",
       " 'damage': 132,\n",
       " 'wais': 4,\n",
       " 'fury': 6,\n",
       " 'wnv': 1,\n",
       " 'fizzle': 1,\n",
       " 'bdftools': 1,\n",
       " 'psychoenergetic': 1,\n",
       " 'rogerw': 3,\n",
       " 'educate': 21,\n",
       " 'fulfulling': 1,\n",
       " 'jfare': 5,\n",
       " 'garner': 8,\n",
       " 'stable': 64,\n",
       " 'oneida': 1,\n",
       " 'working': 483,\n",
       " 'calls': 207,\n",
       " 'discusion': 6,\n",
       " 'umkjdj': 1,\n",
       " 'heating': 19,\n",
       " 'fairgrove': 1,\n",
       " 'partway': 1,\n",
       " 'anaphylactic': 4,\n",
       " 'hype': 31,\n",
       " 'jabber': 1,\n",
       " 'yns': 1,\n",
       " 'whitey': 2,\n",
       " 'persay': 1,\n",
       " 'expectedly': 1,\n",
       " 'kuyers': 1,\n",
       " 'massachussets': 1,\n",
       " 'rightness': 2,\n",
       " 'nokes': 6,\n",
       " 'msll': 1,\n",
       " 'cared': 18,\n",
       " 'deskwriter': 5,\n",
       " 'automatic': 108,\n",
       " 'constitutions': 13,\n",
       " 'afternoon': 43,\n",
       " 'btw': 273,\n",
       " 'unsettling': 2,\n",
       " 'dcw': 2,\n",
       " 'pkeenan': 1,\n",
       " 'gentleman': 14,\n",
       " 'inverted': 4,\n",
       " 'mamishev': 2,\n",
       " 'aepc': 1,\n",
       " 'antifungals': 1,\n",
       " 'beetle': 5,\n",
       " 'dovetail': 3,\n",
       " 'elastomers': 1,\n",
       " 'cites': 24,\n",
       " 'mwm': 38,\n",
       " 'myxkg': 1,\n",
       " 'polished': 9,\n",
       " 'turkler': 2,\n",
       " 'torkel': 1,\n",
       " 'wynn': 3,\n",
       " 'composing': 3,\n",
       " 'smartdrive': 9,\n",
       " 'djcoyle': 1,\n",
       " 'pitchout': 1,\n",
       " 'succubi': 1,\n",
       " 'ranheim': 3,\n",
       " 'backo': 1,\n",
       " 'ylnth': 1,\n",
       " 'blockiness': 3,\n",
       " 'poljot': 1,\n",
       " 'formula': 24,\n",
       " 'summarizes': 10,\n",
       " 'sportswriters': 6,\n",
       " 'rifling': 1,\n",
       " 'kg': 25,\n",
       " 'dalcs': 4,\n",
       " 'lsq': 1,\n",
       " 'seefeld': 3,\n",
       " 'importing': 12,\n",
       " 'investigation': 69,\n",
       " 'metaphorically': 4,\n",
       " 'barring': 11,\n",
       " 'turret': 1,\n",
       " 'idol': 3,\n",
       " 'kleriklemek': 1,\n",
       " 'stefano': 1,\n",
       " 'quilty': 3,\n",
       " 'industru': 1,\n",
       " 'reactor': 19,\n",
       " 'vermeille': 1,\n",
       " 'populated': 11,\n",
       " 'speculator': 1,\n",
       " 'racked': 2,\n",
       " 'yub': 1,\n",
       " 'chihuahua': 2,\n",
       " 'negotiable': 17,\n",
       " 'disbanded': 4,\n",
       " 'cedric': 1,\n",
       " 'fixer': 2,\n",
       " 'minerals': 8,\n",
       " 'revenge': 28,\n",
       " 'imperialism': 3,\n",
       " 'handheld': 10,\n",
       " 'hesd': 1,\n",
       " 'sequiter': 1,\n",
       " 'unimpressive': 1,\n",
       " 'timescales': 1,\n",
       " 'relapse': 1,\n",
       " 'mitsubushi': 1,\n",
       " 'consumerism': 1,\n",
       " 'transactional': 1,\n",
       " 'byw': 1,\n",
       " 'beal': 7,\n",
       " 'sniffing': 3,\n",
       " 'pixmap': 37,\n",
       " 'singularly': 1,\n",
       " 'nrp': 5,\n",
       " 'talatinian': 1,\n",
       " 'drinckes': 1,\n",
       " 'mjuric': 1,\n",
       " 'mccullou': 10,\n",
       " 'cq': 5,\n",
       " 'benzocaine': 1,\n",
       " 'forms': 92,\n",
       " 'merkurs': 1,\n",
       " 'feel': 522,\n",
       " 'leafing': 1,\n",
       " 'superiour': 1,\n",
       " 'logging': 10,\n",
       " 'kenyon': 2,\n",
       " 'bigdog': 2,\n",
       " 'vica': 2,\n",
       " 'anvance': 1,\n",
       " 'nasty': 51,\n",
       " 'psychadelic': 1,\n",
       " 'wirklich': 1,\n",
       " 'unusual': 82,\n",
       " 'ulkemizin': 1,\n",
       " 'schenectady': 6,\n",
       " 'flanger': 1,\n",
       " 'reservest': 1,\n",
       " 'xnote': 1,\n",
       " 'mylar': 7,\n",
       " 'spokesperson': 10,\n",
       " 'list': 620,\n",
       " 'cgpp': 1,\n",
       " 'jkjec': 5,\n",
       " 'tendonitis': 2,\n",
       " 'mbytes': 3,\n",
       " 'unscientific': 5,\n",
       " 'modulated': 5,\n",
       " 'socialized': 7,\n",
       " 'fij': 8,\n",
       " 'scandinavia': 5,\n",
       " 'enjoyable': 5,\n",
       " 'silliness': 2,\n",
       " 'pooled': 1,\n",
       " 'techbooks': 2,\n",
       " 'trafic': 2,\n",
       " 'quadrinomials': 1,\n",
       " 'lsbfirst': 1,\n",
       " 'disassociates': 1,\n",
       " 'international': 261,\n",
       " 'mine': 441,\n",
       " 'muesums': 1,\n",
       " 'financiers': 1,\n",
       " 'iir': 1,\n",
       " 'gwu': 1,\n",
       " 'quantum': 63,\n",
       " 'nis': 2,\n",
       " 'tranmit': 2,\n",
       " 'polytheism': 1,\n",
       " 'clarifying': 1,\n",
       " 'lyf': 1,\n",
       " 'avakian': 1,\n",
       " 'pagemaker': 10,\n",
       " 'anyhows': 1,\n",
       " 'alsys': 3,\n",
       " 'exempting': 2,\n",
       " 'arglebargle': 1,\n",
       " 'hamish': 1,\n",
       " 'jbatka': 1,\n",
       " 'hops': 2,\n",
       " 'unreasonably': 5,\n",
       " 'rallied': 1,\n",
       " 'futurenet': 2,\n",
       " 'inlcuded': 2,\n",
       " 'svbgt': 1,\n",
       " 'hydromechanics': 1,\n",
       " 'energized': 1,\n",
       " 'philosophers': 12,\n",
       " 'emg': 3,\n",
       " 'agitating': 1,\n",
       " 'gonorrhoeae': 1,\n",
       " 'schriever': 2,\n",
       " 'wheaties': 1,\n",
       " 'resulted': 41,\n",
       " 'qadian': 1,\n",
       " 'issst': 3,\n",
       " 'zvt': 1,\n",
       " 'instone': 1,\n",
       " 'paralleled': 2,\n",
       " 'pacha': 1,\n",
       " 'standpoint': 18,\n",
       " 'mitubishi': 1,\n",
       " 'evenzo': 1,\n",
       " 'echoes': 5,\n",
       " 'vanderby': 10,\n",
       " 'plugging': 8,\n",
       " 'guykuo': 9,\n",
       " 'teritory': 4,\n",
       " 'superstitions': 1,\n",
       " 'khojali': 2,\n",
       " 'structural': 14,\n",
       " 'gamble': 3,\n",
       " 'bzh': 1,\n",
       " 'nationally': 10,\n",
       " 'seth': 28,\n",
       " 'newsworld': 1,\n",
       " 'digging': 14,\n",
       " 'feeble': 7,\n",
       " 'voa': 1,\n",
       " 'mathematically': 12,\n",
       " 'expensively': 1,\n",
       " 'eih': 1,\n",
       " 'vbkj': 1,\n",
       " 'chalkstone': 1,\n",
       " 'scuba': 1,\n",
       " 'carburator': 1,\n",
       " 'mensch': 1,\n",
       " 'hri': 2,\n",
       " 'cowell': 1,\n",
       " 'ekdfc': 3,\n",
       " 'acting': 58,\n",
       " 'trivializers': 1,\n",
       " 'leono': 1,\n",
       " 'theophilus': 1,\n",
       " 'zarmooni': 1,\n",
       " 'reunions': 2,\n",
       " 'dimmer': 5,\n",
       " 'amantadine': 1,\n",
       " 'finchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchm': 1,\n",
       " 'fingertip': 1,\n",
       " 'gruss': 2,\n",
       " 'muflers': 1,\n",
       " 'tons': 36,\n",
       " 'altitude': 30,\n",
       " 'immunocompromised': 4,\n",
       " 'infmx': 3,\n",
       " 'dms': 6,\n",
       " 'jlpicard': 1,\n",
       " 'transaxles': 1,\n",
       " 'quits': 2,\n",
       " 'munday': 1,\n",
       " 'physican': 2,\n",
       " 'salamen': 1,\n",
       " 'neils': 1,\n",
       " 'idct': 1,\n",
       " 'ibrahimoglu': 1,\n",
       " 'cased': 1,\n",
       " 'unjustified': 5,\n",
       " 'carte': 1,\n",
       " 'sevr': 1,\n",
       " 'blass': 1,\n",
       " 'meer': 2,\n",
       " 'yhvh': 1,\n",
       " 'sprung': 3,\n",
       " 'knbc': 1,\n",
       " 'grf': 6,\n",
       " 'ecclesiological': 1,\n",
       " 'subtracted': 2,\n",
       " 'theologica': 4,\n",
       " 'nuggets': 4,\n",
       " 'lightsails': 1,\n",
       " 'cracker': 8,\n",
       " 'segard': 6,\n",
       " 'hauling': 1,\n",
       " 'hypothesis': 34,\n",
       " 'isrmc': 2,\n",
       " 'nui': 3,\n",
       " 'popped': 14,\n",
       " 'northfield': 1,\n",
       " 'gregp': 1,\n",
       " 'gulls': 3,\n",
       " 'eacj': 1,\n",
       " 'ecosystems': 1,\n",
       " 'mlk': 11,\n",
       " 'edt': 44,\n",
       " 'cents': 24,\n",
       " 'ctrl': 11,\n",
       " 'fences': 4,\n",
       " 'obviousness': 1,\n",
       " 'ys': 8,\n",
       " 'bacalzo': 1,\n",
       " 'xakellis': 1,\n",
       " 'hilger': 1,\n",
       " 'bain': 6,\n",
       " 'satie': 1,\n",
       " 'euax': 1,\n",
       " 'pcarmack': 2,\n",
       " 'salerno': 1,\n",
       " 'fastmicro': 4,\n",
       " 'isas': 4,\n",
       " 'kosher': 12,\n",
       " 'victories': 10,\n",
       " 'creationist': 8,\n",
       " ...}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary containing the frequency of words utilizing the 'frequency_dict' function\n",
    "\n",
    "# Expect this chunk to take a comparatively longer time to execute since our dataset is large\n",
    "\n",
    "freq_word = frequency_dict(lines)\n",
    "\n",
    "freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "vLvPijR_GKHn"
   },
   "outputs": [],
   "source": [
    "# Create a function to calculate the Term Frequency\n",
    "\n",
    "def term_frequency(document, word):\n",
    "    '''\n",
    "    document: list containing the entire corpus\n",
    "    word: word whose term frequency is to be calculated\n",
    "    ---\n",
    "    tf: returns term frequency value\n",
    "    '''\n",
    "#     count = 0\n",
    "#     for token in document:\n",
    "#         if token == word:\n",
    "#             count = count + 1\n",
    "    \n",
    "#     tf = count/len(document)\n",
    "            \n",
    "#     return tf\n",
    "\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "HA99G_yAGLCC"
   },
   "outputs": [],
   "source": [
    "# Create a function to calculate the Inverse Document Frequency\n",
    " \n",
    "def inverse_df(word):\n",
    "    '''\n",
    "    word: word whose inverse document frequency is to be calculated\n",
    "    ---\n",
    "    idf: return inverse document frequency value\n",
    "    '''\n",
    "\n",
    "#     idf = math.log(total_docs / freq_word[word])\n",
    "#     return idf\n",
    "\n",
    "    try:\n",
    "        word_occurance = freq_word[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_documents/word_occurance + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "F0irgwv2GRfE"
   },
   "outputs": [],
   "source": [
    "#Create a function to combine the term frequencies (TF) and inverse document (IDF) frequencies calculated above to get TF-IDF\n",
    "\n",
    "def tfidf(sentence,dict_idx):\n",
    "    '''\n",
    "    sentence: list containing the entire corpus\n",
    "    dict: dictionary keeping track of index of each word\n",
    "    ---\n",
    "    tf_idf_vec: returns computed tf-idf\n",
    "    '''\n",
    "\n",
    "#     for word in dict_idx:        \n",
    "#         tf_idf_vec = term_frequency(sentence, word) * inverse_df(word)      \n",
    "#     return tf_idf_vec\n",
    "\n",
    "    tf_idf_vec = np.zeros((len(word_list),))\n",
    "    for word in sentence:\n",
    "        tf = term_frequency(sentence,word)\n",
    "        idf = inverse_df(word)       \n",
    "        value = tf*idf\n",
    "        tf_idf_vec[dict_idx[word]] = value \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "_VKJhqatGWpV"
   },
   "outputs": [],
   "source": [
    "#Compute the vectors utilizing the 'tfidf' function created above to obtain a TF-IDF Encoded text corpus\n",
    "tf_idf_df = []\n",
    "#print(lines)\n",
    "for each_text in lines:\n",
    "#    print(each_text)\n",
    "    tf_idf_df.append (tfidf(each_text,dict_idx))\n",
    "    \n",
    "tf_idf_df = pd.DataFrame(tf_idf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE0UGUaSGb8I"
   },
   "source": [
    "## Multinomial Naive Bayes (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "yWYcxrdJGfDC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1      2      3      4      5      6      7      8      9      \\\n",
      "0        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "11309    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11310    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11311    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11312    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11313    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "       ...  73233  73234  73235  73236  73237  73238  73239  73240  73241  \\\n",
      "0      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "11309  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11310  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11311  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11312  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11313  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "       73242  \n",
      "0        0.0  \n",
      "1        0.0  \n",
      "2        0.0  \n",
      "3        0.0  \n",
      "4        0.0  \n",
      "...      ...  \n",
      "11309    0.0  \n",
      "11310    0.0  \n",
      "11311    0.0  \n",
      "11312    0.0  \n",
      "11313    0.0  \n",
      "\n",
      "[11314 rows x 73243 columns]\n"
     ]
    }
   ],
   "source": [
    "#Fit a Multinomial Naive Bayes Model on our dataset\n",
    "#np_array = np.array(tf_idf_vec)\n",
    "print(tf_idf_df)\n",
    "model = MultinomialNB().fit(tf_idf_df, mydata_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "G6CiQB4qGfqH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 4 ... 3 1 8]\n"
     ]
    }
   ],
   "source": [
    "#Perform testing on the train dataset\n",
    "\n",
    "pred = model.predict(tf_idf_df)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "yCLagGu6Gh6T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.8883683931412409\n",
      "Accuracy:  0.8883683931412409\n"
     ]
    }
   ],
   "source": [
    "#Calculate the F1 Score and the Accuracy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "F1_score = f1_score(mydata_train.target, pred, average='micro')\n",
    "Accuracy = metrics.accuracy_score(mydata_train.target, pred)\n",
    "print(\"F1 Score: \", F1_score)\n",
    "print(\"Accuracy: \", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbMRqJv5Gl2F"
   },
   "source": [
    "### Expected Output:\n",
    "F1 Score: 0.9533633964397735\n",
    "\n",
    "Accuracy: 0.9524482941488421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWRDuUqU-taV"
   },
   "source": [
    "Your accuracy does not have to be exactly the same. This is just to give you an estimate of what could you expect your accuracy to be around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfMc8cz93Cc0"
   },
   "source": [
    "## Question 2 Vector Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70iwEeL23F7K"
   },
   "source": [
    "In this unsupervised learning task we are going to cluster wikipedia articles into groups using T-SNE visualization after vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHx4YuxW36oM"
   },
   "source": [
    "### Collect articles from Wikipedia (10 points)\n",
    "\n",
    "In this section we will download articles from wikipedia and then vectorize them in the next step. You can select somewhat related topics or fetch the articles randomly. \n",
    "(Use dir() and help() functions or refer wikipedia documentation)\n",
    "You may also pick any other data source of your choice instead of wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "jA419x6__mjg"
   },
   "outputs": [],
   "source": [
    "# install libraries\n",
    "#pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vLMLk4K84Zbn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "['Natural language processing', 'Natural language', 'Outline of natural language processing', 'Natural-language understanding', 'History of natural language processing', 'Natural Language Toolkit', 'Natural-language user interface', 'Natural-language programming', 'Empirical Methods in Natural Language Processing', 'Process']\n",
      "['Machine learning', 'Hyperparameter (machine learning)', 'Active learning (machine learning)', 'Boosting (machine learning)', 'Quantum machine learning', 'Adversarial machine learning', 'Attention (machine learning)', 'Deep learning', 'Automated machine learning', 'Support-vector machine']\n",
      "['Quantum machine learning', 'Machine learning in physics', 'Quantum computing', 'Cambridge Quantum Computing', 'Quantum neural network', 'Xanadu Quantum Technologies', 'Outline of machine learning', 'Quantum technology', 'Quantum geometry', 'List of companies involved in quantum computing or communication']\n",
      "['Artificial intelligence', 'Artificial general intelligence', 'A.I. Artificial Intelligence', 'History of artificial intelligence', 'Swarm intelligence', 'Explainable artificial intelligence', 'Applications of artificial intelligence', 'Philosophy of artificial intelligence', 'Artificial intelligence in healthcare', 'Ethics of artificial intelligence']\n",
      "['Data science', 'Data analysis', 'Data type', 'Data', 'Master in Data Science', 'Branches of science', 'Data structure', 'Computer science', 'Big data', 'Open data']\n",
      "['Master in Data Science', 'Master of Science in Information Technology', \"List of master's degrees\", 'Master of Library and Information Science', \"List of master's degrees in North America\", 'Master of Science in Business Analytics', 'MicroMasters', 'Jenny Bryan', 'Master of Business Administration', 'Master of Finance']\n",
      "['Analytics', 'Data analysis', 'Big data', 'Google Analytics', 'Predictive analytics', 'Business analytics', 'Prescriptive analytics', 'Web analytics', 'Verisk Analytics', 'Data Analytics Library']\n",
      "['Data mining', 'Lift (data mining)', 'Examples of data mining', 'Educational data mining', 'Data stream mining', 'Oracle Data Mining', 'Text mining', 'Data Mining Extensions', 'Wrapper (data mining)', 'Web mining']\n",
      "['Information retrieval', 'Precision and recall', 'Relevance (information retrieval)', 'Evaluation measures (information retrieval)', 'Private information retrieval', 'Ranking (information retrieval)', 'Thesaurus (information retrieval)', 'Multimedia information retrieval', 'Music information retrieval', 'Legal information retrieval']\n",
      "['Robotics', 'Robot', 'Three Laws of Robotics', 'Industrial robot', 'USRobotics', 'Robotics;Notes', 'Rethink Robotics', 'Evolution Robotics', 'FIRST Robotics Competition', 'Humanoid robot']\n",
      "['Data and information visualization', 'Biological data visualization', 'Visualization (graphics)', 'Glyph (data visualization)', 'Scientific visualization', 'Visualization', 'Dundas Data Visualization', 'Infographic', 'Exploratory data analysis', 'Google Data Studio']\n",
      "['Document classification', 'Naive Bayes classifier', 'FastText', 'List of datasets for machine-learning research', 'Classification of demons', 'Hittite inscriptions', 'Text mining', 'Text nailing', 'Statistical classification', 'Taxonomy (biology)']\n",
      "['Bank of America', 'Bank of America Stadium', 'Bank of America Private Bank', 'Bank of America Tower (Manhattan)', 'American Express', 'Bank of America Tower', 'Bank of America Home Loans', 'Bank of America Corporate Center', 'Bank', 'Bank of North America']\n",
      "['Visa Inc.', 'Visa', 'Visa Debit', '3-D Secure', 'List of most valuable brands', 'Alfred F. Kelly Jr.', 'Verisk Analytics', 'Mastercard', 'Visa Electron', 'Plus (interbank network)']\n",
      "['European Central Bank', 'Central bank', 'Central banks and currencies of Europe', 'European System of Central Banks', 'President of the European Central Bank', 'Seat of the European Central Bank', 'Central and Eastern Europe', 'Central Bank of Ireland', 'Executive Board of the European Central Bank', 'Central bank digital currency']\n",
      "['Financial technology', 'OneConnect Financial Technology', 'Finance', 'Financial services', 'Financial technology in India', 'Financial Technologies Group', 'Broadridge Financial Solutions', 'SS&C Technologies', 'Center of Financial Technologies', 'Technology']\n",
      "['International Monetary Fund', 'Pakistan and the International Monetary Fund', 'South Korea and the International Monetary Fund', 'Ukraine and the International Monetary Fund', 'Romania and the International Monetary Fund', 'Chief Economist of the International Monetary Fund', 'Myanmar and the International Monetary Fund', 'Cambodia and the International Monetary Fund', 'Indonesia and the International Monetary Fund', 'China and the International Monetary Fund']\n",
      "['J. P. Morgan', 'J.P. Morgan & Co.', 'J. P. Morgan Jr.', 'JPMorgan Chase', 'J. P. Morgan (disambiguation)', 'Morgan Library & Museum', 'J.P. Morgan Reserve Card', 'J.P. Morgan in the United Kingdom', 'Junius Spencer Morgan', '23 Wall Street']\n",
      "['Goldman Sachs', 'Goldman Sachs controversies', 'GoldmanSachs family', 'Dina Powell', 'Samuel Sachs', 'Marcus Goldman', '200 West Street', 'Lloyd Blankfein', 'David M. Solomon', 'Goldman Sachs Foundation']\n",
      "['First Tech Federal Credit Union', 'Technology Credit Union', 'List of credit unions in the United States', 'HP Garage', 'List of companies based in Oregon', 'List of cooperatives', 'Meridian Credit Union', 'Tektronix', 'Credit unions in the United States', 'Communication Workers Union (United Kingdom)']\n",
      "['HSBC', 'HSBC Bank USA', 'HSBC Bank India', 'HSBC (Hong Kong)', 'HSBC (United Kingdom)', 'HSBC Bank', 'HSBC Bank Malaysia', 'HSBC Bank (China)', 'HSBC Bank Malta', 'HSBC Bank Canada']\n",
      "['State Bank of India', 'Public sector banks in India', 'List of banks in India', 'List of chairmen of the State Bank of India', 'Banking in India', 'Central Bank of India', 'Bank of India', 'SBM Bank India', 'Union Bank of India', 'List of governors of the Reserve Bank of India']\n",
      "['Deutsche Bank', 'Deutsche Bank Center', 'Deutsche Bank Building', 'List of banks in Europe', 'Deutsche Bank (Italy)', 'Anshu Jain', 'Deutsche Bank Place', 'Global ATM Alliance', 'Waldstadion (Frankfurt)', 'Deutsche Bank Twin Towers']\n",
      "['HDFC Bank', 'HDFC securities', 'HDFC Life', 'Axis Bank', 'Yes Bank', 'Housing Development Finance Corporation', 'Kotak Mahindra Bank', 'HDFC', 'State Bank of India', 'Aditya Puri']\n",
      "['Barclays', 'Barclays Center', 'Roger Jenkins (banker)', 'Barclay', 'Bob Diamond (banker)', 'Premier League', 'Premier League Player of the Season', 'David and Frederick Barclay', 'Amanda Staveley', 'First Capital Bank Zimbabwe Limited']\n",
      "['United Overseas Bank', 'Indian Overseas Bank', 'DBS Bank', 'UOB Plaza', 'List of banks in Malaysia', 'Wee Cho Yaw', 'Overseas Chinese banks', 'List of banks in India', 'List of largest banks in Southeast Asia', 'List of banks in Thailand']\n",
      "['Standard Chartered', 'Standard Chartered Hong Kong', 'Standard Chartered Bangladesh', 'Standard Chartered Pakistan', 'Standard Chartered Bank Nepal', 'Standard Chartered Bank Building', 'Standard Chartered Singapore', 'Standard Bank', 'Chartered Bank of India, Australia and China', 'Standard Bank (historic)']\n",
      "['Basketball', 'National Basketball Association', \"Thailand men's national basketball team\", 'Basketball at the Southeast Asian Games', 'FIBA Under-19 Basketball World Cup', 'College basketball', 'Basketball at the Asian Games', 'History of basketball', 'Small forward', 'List of basketball arenas']\n",
      "['Swimming', 'Swimming (sport)', 'Synchronized swimming', 'List of top Olympic gold medalists in swimming', 'Nude swimming', 'Swimming pool (disambiguation)', 'Swimming at the Summer Olympics', 'Swimming at the 2017 Southeast Asian Games', 'Aquatics at the 1990 Commonwealth Games', 'Freestyle swimming']\n",
      "['Tennis', 'Table tennis', \"Open Era tennis records  Men's singles\", 'History of tennis', 'French Open', 'Lists of tennis records and statistics', 'Grand Slam (tennis)', 'List of female tennis players', 'Rafael Nadal', 'Glossary of tennis terms']\n",
      "['Football', 'College football', 'Association football', 'Australian rules football', 'American football', 'Gaelic football', 'Defender (association football)', 'England national football team', 'Belgium national football team', 'Kit (association football)']\n",
      "['Association football', 'The Football Association', 'History of association football', 'FIFA', 'Defender (association football)', 'Treble (association football)', 'Association football positions', 'Football', 'Captain (association football)', 'List of association football rivalries']\n",
      "['Cricket', 'India national cricket team', 'Glossary of cricket terms', 'Cricket World Cup', 'Melbourne Cricket Ground', '2023 Cricket World Cup', 'Pakistan Cricket Board', 'Test cricket', 'England cricket team', 'ESPNcricinfo']\n",
      "['Badminton', 'India national badminton team', 'Denmark national badminton team', 'Badminton at the Southeast Asian Games', 'Badminton World Federation', 'Badminton in India', '2022 Thailand Open (badminton)', \"Badminton at the 2019 Southeast Asian Games  Men's team\", 'Badminton Asia Championships', 'BWF World Championships']\n",
      "['Kabaddi', 'Kabaddi Kabaddi Kabaddi', 'Kabaddi Kabaddi', 'Pro Kabaddi League', 'Kabaddi Kabaddi (2003 film)', 'Kabaddi in India', 'Kabaddi Kabaddi (2015 film)', 'Punjabi kabaddi', 'Anup Kumar (kabaddi)', 'Kabaddi World Cup (Standard style)']\n",
      "['Table tennis', 'International Table Tennis Federation', 'World Table Tennis Championships', 'Table tennis at the Southeast Asian Games', 'Table tennis at the 2022 Commonwealth Games', 'Table tennis at the 2021 Southeast Asian Games', 'Table tennis styles', 'Table tennis at the Summer Olympics', 'Glossary of table tennis', 'Table tennis at the 2019 Southeast Asian Games']\n",
      "['Natural language processing', 'Natural language', 'Outline of natural language processing', 'Natural-language understanding', 'History of natural language processing', 'Natural Language Toolkit', 'Natural-language user interface', 'Natural-language programming', 'Empirical Methods in Natural Language Processing', 'Process']\n",
      "['Machine learning', 'Hyperparameter (machine learning)', 'Active learning (machine learning)', 'Boosting (machine learning)', 'Quantum machine learning', 'Adversarial machine learning', 'Attention (machine learning)', 'Deep learning', 'Automated machine learning', 'Support-vector machine']\n",
      "['Quantum machine learning', 'Machine learning in physics', 'Quantum computing', 'Cambridge Quantum Computing', 'Quantum neural network', 'Xanadu Quantum Technologies', 'Outline of machine learning', 'Quantum technology', 'Quantum geometry', 'List of companies involved in quantum computing or communication']\n",
      "['Artificial intelligence', 'Artificial general intelligence', 'A.I. Artificial Intelligence', 'History of artificial intelligence', 'Swarm intelligence', 'Explainable artificial intelligence', 'Applications of artificial intelligence', 'Philosophy of artificial intelligence', 'Artificial intelligence in healthcare', 'Ethics of artificial intelligence']\n",
      "['Data science', 'Data analysis', 'Data type', 'Data', 'Master in Data Science', 'Branches of science', 'Data structure', 'Computer science', 'Big data', 'Open data']\n",
      "['Master in Data Science', 'Master of Science in Information Technology', \"List of master's degrees\", 'Master of Library and Information Science', \"List of master's degrees in North America\", 'Master of Science in Business Analytics', 'MicroMasters', 'Jenny Bryan', 'Master of Business Administration', 'Master of Finance']\n",
      "['Analytics', 'Data analysis', 'Big data', 'Google Analytics', 'Predictive analytics', 'Business analytics', 'Prescriptive analytics', 'Web analytics', 'Verisk Analytics', 'Data Analytics Library']\n",
      "['Data mining', 'Lift (data mining)', 'Examples of data mining', 'Educational data mining', 'Data stream mining', 'Oracle Data Mining', 'Text mining', 'Data Mining Extensions', 'Wrapper (data mining)', 'Web mining']\n",
      "['Information retrieval', 'Precision and recall', 'Relevance (information retrieval)', 'Evaluation measures (information retrieval)', 'Private information retrieval', 'Ranking (information retrieval)', 'Thesaurus (information retrieval)', 'Multimedia information retrieval', 'Music information retrieval', 'Legal information retrieval']\n",
      "['Robotics', 'Robot', 'Three Laws of Robotics', 'Industrial robot', 'USRobotics', 'Robotics;Notes', 'Rethink Robotics', 'Evolution Robotics', 'FIRST Robotics Competition', 'Humanoid robot']\n",
      "['Data and information visualization', 'Biological data visualization', 'Visualization (graphics)', 'Glyph (data visualization)', 'Scientific visualization', 'Visualization', 'Dundas Data Visualization', 'Infographic', 'Exploratory data analysis', 'Google Data Studio']\n",
      "['Document classification', 'Naive Bayes classifier', 'FastText', 'List of datasets for machine-learning research', 'Classification of demons', 'Hittite inscriptions', 'Text mining', 'Text nailing', 'Statistical classification', 'Taxonomy (biology)']\n",
      "['Bank of America', 'Bank of America Stadium', 'Bank of America Private Bank', 'Bank of America Tower (Manhattan)', 'American Express', 'Bank of America Tower', 'Bank of America Home Loans', 'Bank of America Corporate Center', 'Bank', 'Bank of North America']\n",
      "['Visa Inc.', 'Visa', 'Visa Debit', '3-D Secure', 'List of most valuable brands', 'Alfred F. Kelly Jr.', 'Verisk Analytics', 'Mastercard', 'Visa Electron', 'Plus (interbank network)']\n",
      "['European Central Bank', 'Central bank', 'Central banks and currencies of Europe', 'European System of Central Banks', 'President of the European Central Bank', 'Seat of the European Central Bank', 'Central and Eastern Europe', 'Central Bank of Ireland', 'Executive Board of the European Central Bank', 'Central bank digital currency']\n",
      "['Financial technology', 'OneConnect Financial Technology', 'Finance', 'Financial services', 'Financial technology in India', 'Financial Technologies Group', 'Broadridge Financial Solutions', 'SS&C Technologies', 'Center of Financial Technologies', 'Technology']\n",
      "['International Monetary Fund', 'Pakistan and the International Monetary Fund', 'South Korea and the International Monetary Fund', 'Ukraine and the International Monetary Fund', 'Romania and the International Monetary Fund', 'Chief Economist of the International Monetary Fund', 'Myanmar and the International Monetary Fund', 'Cambodia and the International Monetary Fund', 'Indonesia and the International Monetary Fund', 'China and the International Monetary Fund']\n",
      "['J. P. Morgan', 'J.P. Morgan & Co.', 'J. P. Morgan Jr.', 'JPMorgan Chase', 'J. P. Morgan (disambiguation)', 'Morgan Library & Museum', 'J.P. Morgan Reserve Card', 'J.P. Morgan in the United Kingdom', 'Junius Spencer Morgan', '23 Wall Street']\n",
      "['Goldman Sachs', 'Goldman Sachs controversies', 'GoldmanSachs family', 'Dina Powell', 'Samuel Sachs', 'Marcus Goldman', '200 West Street', 'Lloyd Blankfein', 'David M. Solomon', 'Goldman Sachs Foundation']\n",
      "['First Tech Federal Credit Union', 'Technology Credit Union', 'List of credit unions in the United States', 'HP Garage', 'List of companies based in Oregon', 'List of cooperatives', 'Meridian Credit Union', 'Tektronix', 'Credit unions in the United States', 'Communication Workers Union (United Kingdom)']\n",
      "['HSBC', 'HSBC Bank USA', 'HSBC Bank India', 'HSBC (Hong Kong)', 'HSBC (United Kingdom)', 'HSBC Bank', 'HSBC Bank Malaysia', 'HSBC Bank (China)', 'HSBC Bank Malta', 'HSBC Bank Canada']\n",
      "['State Bank of India', 'Public sector banks in India', 'List of banks in India', 'List of chairmen of the State Bank of India', 'Banking in India', 'Central Bank of India', 'Bank of India', 'SBM Bank India', 'Union Bank of India', 'List of governors of the Reserve Bank of India']\n",
      "['Deutsche Bank', 'Deutsche Bank Center', 'Deutsche Bank Building', 'List of banks in Europe', 'Deutsche Bank (Italy)', 'Anshu Jain', 'Deutsche Bank Place', 'Global ATM Alliance', 'Waldstadion (Frankfurt)', 'Deutsche Bank Twin Towers']\n",
      "['HDFC Bank', 'HDFC securities', 'HDFC Life', 'Axis Bank', 'Yes Bank', 'Housing Development Finance Corporation', 'Kotak Mahindra Bank', 'HDFC', 'State Bank of India', 'Aditya Puri']\n",
      "['Barclays', 'Barclays Center', 'Roger Jenkins (banker)', 'Barclay', 'Bob Diamond (banker)', 'Premier League', 'Premier League Player of the Season', 'David and Frederick Barclay', 'Amanda Staveley', 'First Capital Bank Zimbabwe Limited']\n",
      "['United Overseas Bank', 'Indian Overseas Bank', 'DBS Bank', 'UOB Plaza', 'List of banks in Malaysia', 'Wee Cho Yaw', 'Overseas Chinese banks', 'List of banks in India', 'List of largest banks in Southeast Asia', 'List of banks in Thailand']\n",
      "['Standard Chartered', 'Standard Chartered Hong Kong', 'Standard Chartered Bangladesh', 'Standard Chartered Pakistan', 'Standard Chartered Bank Nepal', 'Standard Chartered Bank Building', 'Standard Chartered Singapore', 'Standard Bank', 'Chartered Bank of India, Australia and China', 'Standard Bank (historic)']\n",
      "['Basketball', 'National Basketball Association', \"Thailand men's national basketball team\", 'Basketball at the Southeast Asian Games', 'FIBA Under-19 Basketball World Cup', 'College basketball', 'Basketball at the Asian Games', 'History of basketball', 'Small forward', 'List of basketball arenas']\n",
      "['Swimming', 'Swimming (sport)', 'Synchronized swimming', 'List of top Olympic gold medalists in swimming', 'Nude swimming', 'Swimming pool (disambiguation)', 'Swimming at the Summer Olympics', 'Swimming at the 2017 Southeast Asian Games', 'Aquatics at the 1990 Commonwealth Games', 'Freestyle swimming']\n",
      "['Tennis', 'Table tennis', \"Open Era tennis records  Men's singles\", 'History of tennis', 'French Open', 'Lists of tennis records and statistics', 'Grand Slam (tennis)', 'List of female tennis players', 'Rafael Nadal', 'Glossary of tennis terms']\n",
      "['Football', 'College football', 'Association football', 'Australian rules football', 'American football', 'Gaelic football', 'Defender (association football)', 'England national football team', 'Belgium national football team', 'Kit (association football)']\n",
      "['Association football', 'The Football Association', 'History of association football', 'FIFA', 'Defender (association football)', 'Treble (association football)', 'Association football positions', 'Football', 'Captain (association football)', 'List of association football rivalries']\n",
      "['Cricket', 'India national cricket team', 'Glossary of cricket terms', 'Cricket World Cup', 'Melbourne Cricket Ground', '2023 Cricket World Cup', 'Pakistan Cricket Board', 'Test cricket', 'England cricket team', 'ESPNcricinfo']\n",
      "['Badminton', 'India national badminton team', 'Denmark national badminton team', 'Badminton at the Southeast Asian Games', 'Badminton World Federation', 'Badminton in India', '2022 Thailand Open (badminton)', \"Badminton at the 2019 Southeast Asian Games  Men's team\", 'Badminton Asia Championships', 'BWF World Championships']\n",
      "['Kabaddi', 'Kabaddi Kabaddi Kabaddi', 'Kabaddi Kabaddi', 'Pro Kabaddi League', 'Kabaddi Kabaddi (2003 film)', 'Kabaddi in India', 'Kabaddi Kabaddi (2015 film)', 'Punjabi kabaddi', 'Anup Kumar (kabaddi)', 'Kabaddi World Cup (Standard style)']\n",
      "['Table tennis', 'International Table Tennis Federation', 'World Table Tennis Championships', 'Table tennis at the Southeast Asian Games', 'Table tennis at the 2022 Commonwealth Games', 'Table tennis at the 2021 Southeast Asian Games', 'Table tennis styles', 'Table tennis at the Summer Olympics', 'Glossary of table tennis', 'Table tennis at the 2019 Southeast Asian Games']\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    " Generate a list of wikipedia article to cluster \n",
    " You can maintain a static list of titles or generate them randomly using wikipedia library\n",
    " Some topics include:\n",
    " [\"Northeastern Unversity\", \"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \"Data science\", \"Master in Data Science\", \n",
    " \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Bank\", \"Financial technology\",\"International Monetary Fund\", \n",
    " \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"College Football\", \"Association Football\"]\n",
    "\n",
    " You can add more topics from different categories so that we have a diverse dataset to work with. \n",
    " Ex- About 3+ categories(groups), 3+ topics in each category, 3+ articles in each topic\n",
    "'''\n",
    "\n",
    "# selected topics\n",
    "# topics = [\"Yale University\", \"Harvard\", \"Princeton\", \"Massachusetts Institute of Technology\", \"Washington University\",\"University of Texas at Austin\"\n",
    "#           \"University of Cambridge\", \"Seattle\", \"California\", \"Silicon Valley\", \"Chicago\",\"Texas\",\n",
    "#           \"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \n",
    "#           \"Data science\", \"Master in Data Science\",\"Data Analytics\", \"Data Mining\", \"Information Retrieval\",\"Robotics\",\"Database Management System\"\n",
    "#           \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Financial technology\",\"International Monetary Fund\",\"J.P. Morgan\" ,\"Goldman Sachs\"\n",
    "#           \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"Association Football\",\"Cricket\",\"Badminton\", \"Kabaddi\", \"Table Tennis\"]\n",
    "\n",
    "topics = [\"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \n",
    "           \"Data science\", \"Master in Data Science\",\n",
    "          \"Data Analytics\", \"Data Mining\", \n",
    "          \"Information Retrieval\",\"Robotics\",\"Data Visualization\",\"Text Classification\",\n",
    "           \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Financial technology\",\n",
    "          \"International Monetary Fund\",\"J.P. Morgan\" ,\n",
    "          \"Goldman Sachs\", \"First Tech Credit Union\",\"HSBC Bank\", \"State Bank of India\",\"Deutsche Bank\",\n",
    "           \"HDFC Bank\",\n",
    "          \"Barclays\", \"United Overseas Bank\",\n",
    "          \"Standard Chartered Bank\", \n",
    "           \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \n",
    "          \"Association Football\",\"Cricket\",\n",
    "         \"Badminton\", \"Kabaddi\", \"Table Tennis\"\n",
    "         ]\n",
    "print(len(topics))\n",
    "# list of articles to be downloaded\n",
    "articles = []\n",
    "for topic in topics:\n",
    "    print(wikipedia.search(topic))\n",
    "    articles.append(wikipedia.search(topic))\n",
    "# download and store articles (summaries) in this variable  \n",
    "data = []\n",
    "for article in articles:  \n",
    "        print(article)\n",
    "        \n",
    "        summary = wikipedia.summary(article)\n",
    "        data.append(summary)\n",
    "print(len(articles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgpRv7wQ4Dpm"
   },
   "source": [
    "### Cleaning the Data (5 points)\n",
    "In this step you will decide whether to clean the data or not. If you choose to clean, you may utilize the clean function from assignment 1.\n",
    "\n",
    "**Question:** Why are you (not) choosing to clean the data? Think in terms of whether cleaning data will help in the clustering or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnZpDKcaHTGq"
   },
   "source": [
    "**Answer(1-3 sentences):** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to clean the data so that all the relevant words are left for the better clustering to be done.The reason behind choosing to clean data is:\n",
    "\n",
    "Preprocessing or cleaning the data is a data mining technique that transforms raw data into an understandable format. Raw data(real world data) is always incomplete and that data cannot be sent through a model. That would cause certain errors and deviation from getting actual relevance between different datasets . That is why we need to preprocess data before sending through a model.\n",
    "\n",
    "\n",
    "Unused links, stopwords stretch the data unnecessarily and this does not much incorporate much meaning to the basic categorization or topic of the data. On normalising the data Normalization involves adjusting the values in the feature vector so as to measure them on a common scale thus the comparison to sort out and cluster all the relevant, closely and tightly related data can be done more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "lNj53Pxr963N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "#string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "  # From the last assignment\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www.\\S+\", \"\", text)\n",
    "    text_links_removed = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text_cleaned = \" \".join([word for word in re.split('\\W+', text_links_removed)\n",
    "        if word not in stopword])\n",
    "    text = \" \".join([wn.lemmatize(word) for word in re.split('\\W+', text_cleaned)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for text in data:\n",
    "    cleaned_text.append(clean_text(text))    \n",
    "data = cleaned_text    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvRZUpmq-DmT"
   },
   "source": [
    "### Vectorize the articles (5 points)\n",
    "\n",
    "In this step, we will vectorize the text data. You can use TfidfVectorizer() or countVectorizer() from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "gJk8YY89-OU4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.20152048 0.         ... 0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['016', '10', '1000', ..., '', '', ''], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(X.toarray())\n",
    "vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "DAIGlqEuINWA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 1402)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKLvrKHRQaQq"
   },
   "source": [
    "### Sample Output:\n",
    "(36, 1552)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5ZrGrzD_G8d"
   },
   "source": [
    "### Plot Articles (10 points)\n",
    "Now we will try to verify the groups of articles using T-SNE from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "SjcuZBOe-oZq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neo/Library/Python/3.8/lib/python/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/neo/Library/Python/3.8/lib/python/site-packages/sklearn/manifold/_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.manifold import TSNE\n",
    "    \n",
    "\n",
    "# call TSNE() to fit the data\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "index = 0\n",
    "for val in topics:\n",
    "    labels.append(index)\n",
    "    index = index + 1\n",
    "        \n",
    "X_embedded_stack = np.vstack((X_embedded.T, labels )).T\n",
    "X_embedded_df = pd.DataFrame(data = X_embedded_stack, columns = (\"Dim1\", \"Dim2\",\"labels\" ))\n",
    "#12,15,9\n",
    "X_embedded_df[\"Categories\"] = [0, 0, 0, 0, 0, 0, 0,0,0,0,0,0,\n",
    "                              1, 1, 1, 1, 1,1,1,1,1,1,1,1,1,1,1,\n",
    "                              2, 2, 2, 2, 2, 2, 2, 2, 2\n",
    "#                              , 2, 2, \n",
    "#                               3, 3, 3, 3, 3,\n",
    "#                               4, 4, 4, 4, 4, 4, 4, 4\n",
    "                              ]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCY_blxjO1bs"
   },
   "source": [
    "Plot and annotate the points with different markers for different expected groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  87.50521  ,  -26.940138 ,  102.60816  ,    4.216691 ,\n",
       "        -22.983324 ,   56.163982 ,  -88.54636  ,  -24.893923 ,\n",
       "       -106.34136  ,  -56.294964 ,   51.9607   ,  -16.481052 ,\n",
       "          2.4278626,  -65.56568  ,   46.46657  ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded[:,1][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "3ODUA1Vf-rRd",
    "outputId": "325b5db4-60d1-4907-a487-40893e256ee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Vector Visualization')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import matplotlib.patheffects as PathEffects\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "colors = X_embedded_df[\"Categories\"]\n",
    "num_classes = len(np.unique(colors))\n",
    "labels = X_embedded_df[\"labels\"]\n",
    "palette = np.array(sn.color_palette(\"hls\", num_classes))\n",
    "markers =  np.array([\"d\", \"v\", \"s\", \"*\", \"^\"])    \n",
    "f = plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(aspect='equal')\n",
    "sc = ax.scatter(X_embedded[:,0], X_embedded[:,1])\n",
    "plt.xlim(-25, 25)\n",
    "plt.ylim(-25, 25)\n",
    "ax.axis('tight')\n",
    "\n",
    "present_category = 0\n",
    "categories =  X_embedded_df[\"Categories\"].values\n",
    "for idx in range(len(categories)):\n",
    "    category = categories[idx]\n",
    "    if(category == present_category):\n",
    "        plt.plot(X_embedded[idx][0], X_embedded[idx][1],color = palette[category], marker = markers[category], markersize = 10)\n",
    "    else:\n",
    "        present_category = present_category + 1\n",
    "        plt.plot(X_embedded[idx][0], X_embedded[idx][1],color = palette[category], marker = markers[category], markersize = 10)\n",
    "        \n",
    "for i in range(len(labels)):       \n",
    "    plt.annotate(labels[i], (X_embedded[i][0], X_embedded[i][1] + 0.3))\n",
    "\n",
    "\n",
    "ax.set_title('Vector Visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYcEi1EC_UGO"
   },
   "source": [
    "**Question:** Comment about the categorizion done by T-SNE. Do the articles of related topics cluster together? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tde1Wu5HI6AA"
   },
   "source": [
    "**Answer(1-3 sentences):**  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-SNE algorithm could efficiently project complex data sets onto a 2D plane, while the local structure of the data in the original high-dimensional space is preserved as much as possible.It has mapped our multi-dimensional data spanned across 3 different categories and 36 different topics to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, we manage to visualize fewer exploratory data analysis plots of high dimensional data.\n",
    "Even if we can understand the patterns in data and present it on simple charts, it is still difficult for anyone without statistics background to make sense of it. Also, if we have hundreds of features, we have to study thousands of charts before we can make sense of this data.With the help of dimensionality reduction algorithm, we are able to present the data explicitly.\n",
    "\n",
    "My data is spanned across 3 different categories which are Universities, Cities, Data Analytics, Bank and Games.\n",
    "Each of these categories have 12 , 15, 9 different kinds of topics attached to it.\n",
    "We can see in the above scatter plot that all the similar category data are clustered together.\n",
    "T-SNE, this non-linear dimensionality reduction algorithm finds patterns in the data by identifying observed clusters based on similarity of data points with multiple features. But it is not a clustering algorithm it is a dimensionality reduction algorithm. This is because it maps the multi-dimensional data to a lower dimensional space, the input features are no longer identifiable. Thus we cannot make any inference based only on the output of t-SNE. So essentially it is mainly a data exploration and visualization technique.\n",
    "But t-SNE can be used in the process of classification and clustering by using its output as the input feature for other classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2xrJddIpSsd"
   },
   "source": [
    "# Question 3 Building Neural Networks\n",
    "\n",
    "### We are gonna use Emotions Dataset for this task. We need to classify the given text into different kind of emotions like happy,sad,anger etc.., \n",
    "\n",
    "### We are providing train.txt and val.txt files along with this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJYmVbkvphgX"
   },
   "source": [
    "### Library Imports and Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BO-HU-7uorVX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "#string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "  # From the last assignment\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www.\\S+\", \"\", text)\n",
    "    text_links_removed = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text_cleaned = \" \".join([word for word in re.split('\\W+', text_links_removed)\n",
    "        if word not in stopword])\n",
    "    text = \" \".join([wn.lemmatize(word) for word in re.split('\\W+', text_cleaned)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0D5wVjZw7s0"
   },
   "source": [
    "### Q) Importing the datasets and do the necessary cleaning and convert the text into the vectors which are mentioned in the below code blocks. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MOMhmIlGprK9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import the train.txt and val.txt file into pandas dataframe format \n",
    "\n",
    "## Reading the data and removing columns that are not important. \n",
    "\n",
    "# train \n",
    "train = pd.read_csv(\"train.txt\", sep=\";\", header=None, names=['text', 'emotion'])\n",
    "\n",
    "# validation\n",
    "validation = pd.read_csv(\"val.txt\", sep=\";\" , header=None, names=['text', 'emotion'])\n",
    "# and printout the train.shape and validation.shape \n",
    "print(train.shape)\n",
    "print(validation.shape)\n",
    "\n",
    "# self addition\n",
    "# labels_dict = {'sadness':0, 'joy':1, 'love':2, 'anger':3, 'fear':4, 'surprise':5}\n",
    "# train['label'] = train['emotion'].map(labels_dict )\n",
    "# train.head()\n",
    "# expected shape of train dataset is (16000,2) and validation dataset is (2000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PS7K3p7AqAI8"
   },
   "outputs": [],
   "source": [
    "# clean the text in the train and validation dataframes using the clean_text function provided above\n",
    "\n",
    "text = train['text']\n",
    "cleaned_text_train = []\n",
    "# print(train.iloc[0]['text'])\n",
    "for sentence in text:\n",
    "    sentence = clean_text(sentence)\n",
    "    cleaned_text_train.append(sentence)\n",
    "train['text'] = cleaned_text_train  \n",
    "\n",
    "cleaned_text_val = []\n",
    "\n",
    "validation_text = validation['text']\n",
    "for sentence in validation_text:\n",
    "    sentence = clean_text(sentence)\n",
    "    cleaned_text_val.append(sentence)\n",
    "validation['text'] = cleaned_text_val      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GxZC6RIjq3bu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 13457)\n",
      "(16000, 13457)\n",
      "(16000, 2)\n"
     ]
    }
   ],
   "source": [
    "# initialise count vectorizer from sklearn module with default parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit on train dataset and transform both train and validation dataset\n",
    "X = train['text'].values\n",
    "vectorizer.fit(X)\n",
    "X_train_count_vector =  vectorizer.transform(X)\n",
    "print(X_train_count_vector.shape)\n",
    "# count_train_df = pd.DataFrame(X_train_count_vector.toarray())\n",
    "# print(count_train_df.shape)\n",
    "# print(train.shape)\n",
    "val = validation['text'].values\n",
    "X_val_count_vector = vectorizer.fit_transform(val).toarray()\n",
    "\n",
    "# y = train['emotion'].values\n",
    "# train_text, train_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1000)\n",
    "# train_text_fit = vectorizer.fit(train_text)\n",
    "\n",
    "# X_train_count_vector = vectorizer.transform(train_text_fit)\n",
    "\n",
    "\n",
    "# X = validation['text'].values\n",
    "# y = validation['emotion'].values\n",
    "# val_text, val_test, y_train, y_test = train_test_split(X, y, test_size=None, random_state=1000)\n",
    "# val_text_fit = vectorizer.fit(val_text)\n",
    "# X_val_count_vector = vectorizer.transform(val_text_fit)\n",
    "\n",
    "\n",
    "# X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "# val_text = vectorizer.fit(\n",
    "\n",
    "# X_train_count_vector =  vectorizer.transform(train_text)\n",
    "# text'].values)\n",
    "# X_validation_count_vector = vectorizer.transform(val_text)\n",
    "#X_validation_count_vector_df = np.asarray(X_validation_count_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "85nuazEzq3YT"
   },
   "outputs": [],
   "source": [
    "# initialise tfidf vectorizer from sklearn module with default parameter\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# fit on train dataset and transform both train and validation dataset\n",
    "X_train_tfidf_vector = vectorizer.fit_transform(train['text'])\n",
    "X_validation_tfidf_vector = vectorizer.fit_transform(validation['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jgHWAuG-q3Vm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                              didnt feel humiliated        4\n",
       "1  go feeling hopeless damned hopeful around some...        4\n",
       "2          im grabbing minute post feel greedy wrong        0\n",
       "3  ever feeling nostalgic fireplace know still pr...        3\n",
       "4                                    feeling grouchy        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise label encoder from sklearn module\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "# fit on train labels and transform both train and validation labels\n",
    "train['emotion'] = le.fit_transform(train['emotion'])\n",
    "\n",
    "validation['emotion'] = le.fit(validation['emotion'])\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wjdye0tvq3So"
   },
   "outputs": [],
   "source": [
    "# convert the labels into one hot encoding form\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehotencoder = OneHotEncoder(sparse = False, handle_unknown = 'error', drop = 'first')\n",
    "onehotencoder_train_df = pd.DataFrame(onehotencoder.fit_transform(train[['emotion']]))\n",
    "train = train.join(onehotencoder_train_df)\n",
    "onehotencoder_val_df = pd.DataFrame(onehotencoder.fit_transform(validation[['emotion']]))\n",
    "# one_hot_encoded_data_train = pd.get_dummies(train, columns = ['emotion'])\n",
    "\n",
    "# one_hot_encoded_data_val = pd.get_dummies(validation, columns = ['emotion'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjsiH8YOw-Da"
   },
   "source": [
    "### Q) Build the neural networks using tensorflow keras by following the below instructions. Evaluate the model on different metrics and comment your observations. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AQg14bkTq3KB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# complete this linear model in tensorflow\n",
    "def build_model(X):\n",
    "\n",
    "  # layer 1 : input layer\n",
    "  inp = tf.keras.Input((X.shape[0],))\n",
    "  print(inp.shape) \n",
    "  \n",
    "  # layer 2 : add the dense layer with 2048 units and relu activation\n",
    "  x = tf.keras.layers.Dense(2048,input_dim = X.shape[0], activation = \"relu\")(inp)\n",
    "  print(x.shape) \n",
    "  # layer 3 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)  \n",
    "  print(x.shape) \n",
    "  # layer 4 : add the dense layer with 1024 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(1024, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape) \n",
    "  # layer 5 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)  \n",
    "  print(x.shape) \n",
    "  # layer 6 : add the dense layer with 512 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(512, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape)   \n",
    "  # layer 7 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x) \n",
    "  print(x.shape) \n",
    "  # layer 8 : add the dense layer with 256 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(256, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape) \n",
    "  # layer 9 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)\n",
    "  print(x.shape) \n",
    "  # layer 10 : add the dense layer with 128 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(128, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape)   \n",
    "  # layer 11 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)\n",
    "  print(x.shape) \n",
    "  # layer 12 : output layer with units equal to number of classes and activation as softmax\n",
    "  x = tf.keras.layers.Dense(5,activation = 'softmax')(x) \n",
    "  print(x.shape) \n",
    "    \n",
    "  model = tf.keras.models.Model(inputs=inp, outputs=x)\n",
    "  print(inp.shape)\n",
    "  print(x.shape) \n",
    "  opt = tf.keras.optimizers.Adam(learning_rate=1e-03)\n",
    "  # use loss as categorical crossentropy, optimizer as rmsprop and evaluate model on auc,precision,recall,accuracy \n",
    "  model.compile(loss = 'sparse_categorical_crossentropy' , optimizer = opt, metrics = ['sparse_categorical_accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Q71CC1pIsx0O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 22:13:01.041162: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2048)\n",
      "(None, 2048)\n",
      "(None, 1024)\n",
      "(None, 1024)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 128)\n",
      "(None, 128)\n",
      "(None, 5)\n",
      "(None, 16000)\n",
      "(None, 5)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              32770048  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,557,893\n",
      "Trainable params: 35,557,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# call the build_model function and initialize the model\n",
    "model = build_model(X_train_count_vector)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KyAZNgsBsxwo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "16000\n",
      "2000\n",
      "2000\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Python/3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Python/3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 16000), found shape=(None, 13457)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_val_count_vector\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# X_train_count_vector = X_train_count_vector.toarray()\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_count_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_count_vector, train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,validation_data\u001b[38;5;241m=\u001b[39m(X_val_count_vector, validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]),callbacks\u001b[38;5;241m=\u001b[39m[model_checkpoint_callback,lr_schedule] ) \n\u001b[1;32m     18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_count_vector)\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/7t/h643y62j38j54bfnf6lt27rm0000gn/T/__autograph_generated_filey1xy1s0o.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Python/3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Python/3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 16000), found shape=(None, 13457)\n"
     ]
    }
   ],
   "source": [
    "# train and validate the model on the count vectors of text which we have created initially for 10 epochs, \n",
    "# adjust batch size according to your computation power (suggestion use : 8)\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='tutorial.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy',factor=0.1,patience=3)\n",
    "print(train['emotion'].shape[0])\n",
    "print(count_train_df.shape[0])\n",
    "print(validation['emotion'].shape[0])\n",
    "print(X_val_count_vector.shape[0])\n",
    "# X_train_count_vector = X_train_count_vector.toarray()\n",
    "model.fit(X_train_count_vector, train['emotion'])\n",
    "history = model.fit(X_train_count_vector, train['emotion'], epochs=10, batch_size = 8,validation_data=(X_val_count_vector, validation['emotion']),callbacks=[model_checkpoint_callback,lr_schedule] ) \n",
    "y_pred = model.predict(X_train_count_vector)\n",
    "#print(f'The Overall accuracy score is {accuracy_score(X_train_count_vector, y_pred)}')\n",
    "\n",
    "#loss and optimizer\n",
    "# loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# optim = keras.optimizers.Adam(lr=0.001)\n",
    "# metrics = [\"accuracy\"]\n",
    "\n",
    "# model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "\n",
    "# # training\n",
    "# batch_size = 64\n",
    "# epochs = 5\n",
    "\n",
    "# model.fit(X_train_count_vector, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, verbose=2)\n",
    "\n",
    "# # evaulate\n",
    "# model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "nS02IwLCsxmG"
   },
   "outputs": [],
   "source": [
    "# plot train loss vs val loss, train auc vs val auc, train recall vs val recall, train precision vs val precision and train accuracy vs val accuracy and comment your observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "6FHdCcp7wXyw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16000)\n",
      "(None, 2048)\n",
      "(None, 2048)\n",
      "(None, 1024)\n",
      "(None, 1024)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 128)\n",
      "(None, 128)\n",
      "(None, 5)\n",
      "(None, 16000)\n",
      "(None, 5)\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 16000)]           0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 2048)              32770048  \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,557,893\n",
      "Trainable params: 35,557,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# again call the build_model function and initialize the model\n",
    "model = build_model(train['text'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4gB80M6wXvV"
   },
   "outputs": [],
   "source": [
    "# train and validate the model on the tfidf vectors of text which we have created initially for 10 epochs, \n",
    "# adjust batch size according to your computation power (suggestion use : 8)\n",
    "\n",
    "model = build_model(X_train_tfidf_vector)\n",
    "y_pred = model.predict(X_train_tfidf_vector)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'The Overall accuracy score is {accuracy_score(X_train_tfidf_vector['emotion'], y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsnEOHXRwXkv"
   },
   "outputs": [],
   "source": [
    "# plot train loss vs val loss, train auc vs val auc, train recall vs val recall, train precision vs val precision and train accuracy vs val accuracy and comment your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4adDDQWeQyGb"
   },
   "source": [
    "## Question 4 Theory Question  \n",
    "\n",
    "What is the difference between Count Vectorizer, TFIDF, Word2Vec and Glove? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXTRkF6KRB_D"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "**Differences in the properties of TFIDF, Glove and Word2vec:**\n",
    "1)Glove and Word2vec are both unsupervised models for generating word vectors. The difference between them is the mechanism of generating word vectors. The word vectors generated by either of these models can be used for a wide variety of tasks ranging such as\n",
    "\n",
    "    finding words that are semantically similar to a word,\n",
    "    representing a word when it is being input to a downstream model. A word embedding representation of a word captures more information about a word than just a one-hot representation of the word, since the former captures semantic similarity of that word to other words whereas the latter representation of the word is equidistant from all other words.\n",
    "\n",
    "Tf-idf is a scoring scheme for words - that is a measure of how important a word is to a document.\n",
    "\n",
    "From a practical usage standpoint, while tf-idf is a simple scoring scheme and that is its key advantage, word embeddings may be a better choice for most tasks where tf-idf is used, particularly when the task can benefit from the semantic similarity captured by word embeddings (e.g. in information retrieval tasks)\n",
    "\n",
    "**Differences in the properties of TFIDF and Count Vectorizer:**\n",
    "2)TfidfVectorizer and CountVectorizer both are methods for converting text data into vectors as model can process only numerical data.\n",
    "\n",
    "In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n",
    "\n",
    "To overcome this , we use TfidfVectorizer .\n",
    "\n",
    "In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.\n",
    "\n",
    "**Differences in the properties of word2vec and glove:**\n",
    "3)The two models differ in the way they are trained, and hence lead to word vectors with subtly different properties. Glove model is based on leveraging global word to word co-occurance counts leveraging the entire corpus. Word2vec on the other hand leverages co-occurance within local context (neighbouring words).\n",
    "\n",
    "In practice, however, both these models give similar results for many tasks. Factors such as the dataset on which these models are trained, length of the vectors and so on seem to have a bigger impact than the models themselves. For instance, if I am using these models to derive  the features for a medical application, I can significantly improve performance by training on dataset from the medical domain.\n",
    "\n",
    "**Glove model training:**\n",
    "Glove is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each word (the rows), you count how frequently we see this word in some context (the columns) in a large corpus.  The number of contexts is of course large, since it is essentially combinatorial in size.\n",
    "\n",
    "So then we factorize this matrix to yield a lower-dimensional (word x features) matrix, where each row now yields a vector representation for the corresponding word. In general, this is done by minimizing a reconstruction loss. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.\n",
    "\n",
    "**word2vec model training:**\n",
    "Word2Vec is a feed forward neural network based model to find word embeddings. There are two models that are commonly used to train these embeddings: The skip-gram and the CBOW model. \n",
    "\n",
    "The Skip-gram model takes the input as each word in the corpus, sends them to a hidden layer (embedding layer) and from there it predicts the context words. Once trained, the embedding for a particular word is obtained by feeding the word as input and taking the hidden layer value as the final embedding vector. \n",
    "\n",
    "The CBOW (Continuous Bag of Words) model takes the input the context words for the given word, sends them to a hidden layer (embedding layer) and from there it predicts the original word. Once again, after training, the embedding for a particular word is obtained by feeding the word as input and taking the hidden layer value as the final embedding vector. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQQsqcto79zE"
   },
   "source": [
    "What is the significant difference between the Niave Bayes Implementation using Bag of Words and TF-IDF? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtiPCTZM8Aua"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaXBo1Bw8C1K"
   },
   "source": [
    "Drawbacks of using a Bag-of-Words (BoW) Model\n",
    "\n",
    "If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.\n",
    "Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)\n",
    "We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text.\n",
    "\n",
    "Property of TFIDF:\n",
    "    \n",
    "TF-IDF gives larger values for less frequent words and is high when both IDF and TF values are high i.e the word is rare in \n",
    "all the documents combined but frequent in a single document.\n",
    "\n",
    "In a nutshell:\n",
    "Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), \n",
    "while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "Bag of Words vectors are easy to interpret. However, TF-IDF usually performs better in machine learning models.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS6120_NLP_Assignment_2_Notebook",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
