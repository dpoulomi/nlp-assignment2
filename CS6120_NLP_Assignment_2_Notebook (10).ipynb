{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni4T_dwEHFVa"
   },
   "source": [
    "## CS 6120: Natural Language Processing - Prof. Ahmad Uzair\n",
    "\n",
    "### Assignment 2: Text Classification and Neural Network\n",
    "### Total Points: 100 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loQ22M-bCubq"
   },
   "source": [
    "In Assignment 2, you will be dealing with text classification using Multinomial Naive Bayes and Neural Networks. You will also be dealing with vector visualization. In the previous assingment you implemented Bag of Words as the feature selection method. However, in this assignment you will be using TF-IDF Vectorization instead of Bag of Words. We recommend starting with this assignment a little early as the datasets are quite large and several parts of the assignment might take long duration to execute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3brglC1C0mZ"
   },
   "source": [
    "## Question 1 Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ1y75IoC3rE"
   },
   "source": [
    "In the first question you will be dealing with 20 News Group Dataset. You are required to implement TF-IDF vectorization from scratch and perform Multinomial Naive Bayes Classification on the News Group Dataset.\n",
    "You may use appropriate packages or modules for fitting the Multinomial Naive Bayes Model, however, the implementation of the TF-IDF Vectorization should be from the scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRjM45PyC8ML"
   },
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "Link to the original dataset: http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
    "\n",
    "You can also import the dataset from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CmFf2INNDJRb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "AEp1SHe5DKCB"
   },
   "outputs": [],
   "source": [
    "# Import the 20 news group dataset utilizing sklearn library\n",
    "#categories = ['alt.atheism']\n",
    "\n",
    "mydata_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "\n",
    "mydata_test = fetch_20newsgroups(subset='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "OscOmcE0DMHa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Print the news groups(target) in the dataset\n",
    "\n",
    "pprint(list(mydata_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "JvQb2r0aDR_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "<class 'sklearn.utils._bunch.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "# What is the type of 'mydata_train' and 'mydata_test'\n",
    "\n",
    "print(type(mydata_train))\n",
    "print(type(mydata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ozzwyREhDaMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "11314\n",
      "7532\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the data\n",
    "\n",
    "print(len(mydata_train.data))\n",
    "print(len(mydata_train.filenames))\n",
    "print(len(mydata_test.data))\n",
    "print(len(mydata_test.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlipMuEpDz-K"
   },
   "source": [
    "### Expected Output: \n",
    "11314\n",
    "\n",
    "11314\n",
    "\n",
    "7532\n",
    "\n",
    "7532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55FKOBBuEDI2"
   },
   "source": [
    "## Extracting Features from the Dataset                        (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4GDENzmEEkG"
   },
   "source": [
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgxfDXmxEHid"
   },
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qNFFYyEKsa"
   },
   "source": [
    "Our model cannot simply read the text data so we convert it into numerical format. In order to convert the data into numerical format we create vectors from text.\n",
    "\n",
    "For this particular purpose we could either employ Bag of Words or TF-IDF Vectorization\n",
    "\n",
    "Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency, which instead of giving more weight to words that occur more frequently, it gives a higher weight to words that occur less frequently.\n",
    "\n",
    "Ref:https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/#:~:text=Bag%20of%20Words%20just%20creates,less%20important%20ones%20as%20well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLzgRJRZEP3k"
   },
   "source": [
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe5lHi3NE1QJ"
   },
   "source": [
    "Term Frequency is the measure of the frequency of words in a document. It is the ratio of the number of times the word appears in a document compared to the total number of words in that document.\n",
    "\n",
    "The words that occur rarely in the corpus have a high IDF score. It is the log of the ratio of the number of documents to the number of documents containing the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXotAER_EQ8T"
   },
   "source": [
    "idf(t) = log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "2vzVI8ylFAEl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = mydata_train.data\n",
    "test = mydata_test.data\n",
    "# for line in text:\n",
    "#     print (line)\n",
    "#     words = line.split()\n",
    "#     print(words)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2AatmvFtE-"
   },
   "source": [
    "## Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used stemming operation for text-preprocessing\n",
    "def clean_review(review):\n",
    "    no_html = BeautifulSoup(review).get_text()\n",
    "    clean = re.sub(\"[^a-z\\s]+\", \" \", no_html, flags=re.IGNORECASE)\n",
    "    clean = re.sub(\"(\\s+)\", \" \", clean)\n",
    "#     clean = clean.lower()\n",
    "#     stopwords_en = stopwords.words(\"english\")\n",
    "#     cleaned_stopwords = [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]\n",
    "    cleaned_stopwords = clean.split()\n",
    "    stemmed_words = []\n",
    "    for w in cleaned_stopwords:\n",
    "        stemmed_words.append(snowball.stem(w))\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "XyJbe42AFuHp"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "lines = [] \n",
    "word_list = [] \n",
    " \n",
    "# for line in text:\n",
    "#     #tokenize the text documents and update the lists word_list and lines  \n",
    "#     each_line_word_list = clean_review(line)\n",
    "#     lines.append(each_line_word_list)\n",
    "# word_list = list(set(chain(*lines)))                \n",
    "    \n",
    "# # Make sure the word_list contains unique tokens\n",
    "\n",
    "\n",
    "# # Calculate the total documents present in the corpus\n",
    "# total_docs = len(mydata_train.data)\n",
    "\n",
    " \n",
    "# dict_idx = {}\n",
    "# for word in word_list:\n",
    "#     index = word_list.index(word)\n",
    "#     dict_idx[word] = index\n",
    "\n",
    "\n",
    "\n",
    "#copied from net\n",
    "\n",
    "for sent in text:\n",
    "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
    "    lines.append(x)\n",
    "    for word in x:\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "            \n",
    "word_list = set(word_list)\n",
    "total_documents = len(lines)\n",
    "\n",
    "dict_idx = {} #Dictionary to store index for each word\n",
    "i = 0\n",
    "for word in word_list:\n",
    "    dict_idx[word] = i\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "E6M_CoI9FxYb"
   },
   "outputs": [],
   "source": [
    "# Create a frequency dictionary\n",
    " \n",
    "def frequency_dict(lines):\n",
    "    '''\n",
    "    lines: list containing all the tokens\n",
    "    ---\n",
    "    freq_word: returns a dictionary which keeps the count of the number of documents containing the given word\n",
    "    '''\n",
    "#     freq_word = {}\n",
    "#     for each_list in lines:\n",
    "#         for token in each_list:\n",
    "#             if freq_word.get(token) == None :\n",
    "#                 freq_word[token] = 1\n",
    "#             else:\n",
    "#                 freq_word[token] = freq_word[token] + 1 \n",
    "#     return freq_word\n",
    "\n",
    "\n",
    "#copied from internet\n",
    "\n",
    "    freq_word = {}\n",
    "    for word in word_list:\n",
    "        freq_word[word] = 0\n",
    "        for line in lines:\n",
    "            if word in line:\n",
    "                freq_word[word] += 1\n",
    "    return freq_word\n",
    " \n",
    "#word_count = count_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "qFkt9KBgFz43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duckworth': 1,\n",
       " 'srscnslt': 1,\n",
       " 'sammons': 9,\n",
       " 'actaual': 1,\n",
       " 'circulation': 10,\n",
       " 'ydvp': 1,\n",
       " 'jab': 1,\n",
       " 'transylvania': 1,\n",
       " 'mwtilden': 1,\n",
       " 'uaccess': 12,\n",
       " 'privelege': 1,\n",
       " 'etherlink': 1,\n",
       " 'lauches': 1,\n",
       " 'servo': 2,\n",
       " 'millenia': 8,\n",
       " 'lim': 15,\n",
       " 'freeport': 2,\n",
       " 'goudswaard': 3,\n",
       " 'reinstate': 4,\n",
       " 'evidentally': 1,\n",
       " 'legged': 1,\n",
       " 'cashed': 1,\n",
       " 'mro': 1,\n",
       " 'unfortunate': 42,\n",
       " 'whitsell': 5,\n",
       " 'extraction': 6,\n",
       " 'dscy': 2,\n",
       " 'retain': 29,\n",
       " 'pencom': 1,\n",
       " 'acceptably': 3,\n",
       " 'averting': 1,\n",
       " 'costello': 6,\n",
       " 'cherbayev': 1,\n",
       " 'zrcimrck': 1,\n",
       " 'preparation': 20,\n",
       " 'modalistic': 1,\n",
       " 'cleaning': 44,\n",
       " 'cjjj': 1,\n",
       " 'ariane': 8,\n",
       " 'portugal': 7,\n",
       " 'floatingpoint': 1,\n",
       " 'howe': 20,\n",
       " 'mpoly': 2,\n",
       " 'quibble': 11,\n",
       " 'luftmeister': 1,\n",
       " 'tennyson': 1,\n",
       " 'instate': 2,\n",
       " 'xws': 1,\n",
       " 'weevils': 2,\n",
       " 'wilhelm': 2,\n",
       " 'xtndepth': 1,\n",
       " 'ridiculed': 1,\n",
       " 'marblehead': 1,\n",
       " 'pleasant': 32,\n",
       " 'thyssen': 1,\n",
       " 'ujc': 1,\n",
       " 'chartered': 2,\n",
       " 'foolkiller': 2,\n",
       " 'mbyets': 1,\n",
       " 'hashemites': 1,\n",
       " 'boone': 6,\n",
       " 'hearts': 44,\n",
       " 'slumping': 2,\n",
       " 'gandalf': 5,\n",
       " 'flamsteed': 1,\n",
       " 'ld': 30,\n",
       " 'omschrijving': 1,\n",
       " 'jaze': 1,\n",
       " 'high': 716,\n",
       " 'pluck': 1,\n",
       " 'nerves': 17,\n",
       " 'irretrievably': 1,\n",
       " 'detritus': 1,\n",
       " 'classically': 2,\n",
       " 'kratz': 34,\n",
       " 'bmwoa': 2,\n",
       " 'breadtubes': 1,\n",
       " 'sharen': 4,\n",
       " 'sinha': 6,\n",
       " 'micali': 2,\n",
       " 'hhva': 1,\n",
       " 'lifespans': 1,\n",
       " 'neurasthenia': 1,\n",
       " 'honourable': 4,\n",
       " 'hundley': 2,\n",
       " 'martyriologia': 1,\n",
       " 'westbrook': 1,\n",
       " 'riddle': 25,\n",
       " 'fabric': 5,\n",
       " 'gizli': 1,\n",
       " 'bounding': 2,\n",
       " 'yesterdaysdate': 1,\n",
       " 'grendel': 1,\n",
       " 'olmus': 1,\n",
       " 'diagnostic': 21,\n",
       " 'learned': 118,\n",
       " 'amadi': 1,\n",
       " 'digitalequipmentcorporation': 1,\n",
       " 'resemble': 11,\n",
       " 'labeled': 23,\n",
       " 'inquisitors': 1,\n",
       " 'devprogram': 1,\n",
       " 'hetersexual': 1,\n",
       " 'stipulations': 2,\n",
       " 'wonderfully': 5,\n",
       " 'jsut': 2,\n",
       " 'stooke': 2,\n",
       " 'decoders': 5,\n",
       " 'depress': 2,\n",
       " 'renewable': 4,\n",
       " 'thinkers': 5,\n",
       " 'xdeleted': 1,\n",
       " 'brader': 11,\n",
       " 'recoup': 3,\n",
       " 'mitten': 1,\n",
       " 'sigterm': 1,\n",
       " 'yellows': 2,\n",
       " 'middlemen': 1,\n",
       " 'lockheed': 23,\n",
       " 'baltic': 4,\n",
       " 'aeons': 1,\n",
       " 'qdvbgtc': 1,\n",
       " 'mantick': 3,\n",
       " 'comison': 1,\n",
       " 'bmj': 1,\n",
       " 'glutamate': 21,\n",
       " 'pean': 1,\n",
       " 'craziest': 1,\n",
       " 'ixqj': 1,\n",
       " 'shitcom': 1,\n",
       " 'msgptr': 1,\n",
       " 'sunglasses': 8,\n",
       " 'overgeneralization': 1,\n",
       " 'stacy': 1,\n",
       " 'ezel': 1,\n",
       " 'schuster': 2,\n",
       " 'academica': 1,\n",
       " 'heddings': 1,\n",
       " 'guessed': 25,\n",
       " 'whittier': 3,\n",
       " 'sepa': 1,\n",
       " 'spaghetti': 2,\n",
       " 'befoe': 1,\n",
       " 'keats': 1,\n",
       " 'lyall': 6,\n",
       " 'virginity': 2,\n",
       " 'oanb': 1,\n",
       " 'thai': 2,\n",
       " 'belting': 1,\n",
       " 'nucelotide': 1,\n",
       " 'upgrade': 126,\n",
       " 'lawbreaker': 1,\n",
       " 'took': 400,\n",
       " 'npvzi': 1,\n",
       " 'mr': 107,\n",
       " 'hourly': 2,\n",
       " 'pc': 372,\n",
       " 'royal': 64,\n",
       " 'tbwjh': 1,\n",
       " 'chuckle': 12,\n",
       " 'sendkeys': 1,\n",
       " 'jml': 1,\n",
       " 'followinf': 1,\n",
       " 'mho': 4,\n",
       " 'psychologically': 4,\n",
       " 'fledgling': 2,\n",
       " 'switz': 2,\n",
       " 'vilayet': 5,\n",
       " 'downbeat': 1,\n",
       " 'xhrbc': 1,\n",
       " 'wasteland': 1,\n",
       " 'mollica': 1,\n",
       " 'suppression': 3,\n",
       " 'petes': 3,\n",
       " 'rcmd': 1,\n",
       " 'scranton': 2,\n",
       " 'souzexpertisa': 1,\n",
       " 'rubeae': 4,\n",
       " 'laud': 2,\n",
       " 'due': 394,\n",
       " 'investements': 1,\n",
       " 'suicidal': 9,\n",
       " 'monopolism': 1,\n",
       " 'pillsbury': 1,\n",
       " 'yek': 1,\n",
       " 'illegitimately': 1,\n",
       " 'actionm': 1,\n",
       " 'exaggerate': 4,\n",
       " 'hyphae': 4,\n",
       " 'dama': 1,\n",
       " 'typos': 10,\n",
       " 'valuemask': 3,\n",
       " 'uninc': 7,\n",
       " 'hemmorrhoid': 1,\n",
       " 'oftentimes': 3,\n",
       " 'baylor': 11,\n",
       " 'thyagi': 11,\n",
       " 'mmec': 1,\n",
       " 'formally': 16,\n",
       " 'wll': 1,\n",
       " 'feedwater': 2,\n",
       " 'kaltiyin': 1,\n",
       " 'gemeinhart': 1,\n",
       " 'gpk': 1,\n",
       " 'essentially': 129,\n",
       " 'kahn': 10,\n",
       " 'neurovascular': 1,\n",
       " 'unneeded': 1,\n",
       " 'mcgrath': 1,\n",
       " 'dissolution': 3,\n",
       " 'thrice': 3,\n",
       " 'mtz': 2,\n",
       " 'pyschiatrist': 1,\n",
       " 'congenially': 6,\n",
       " 'microstation': 1,\n",
       " 'attractiveness': 2,\n",
       " 'creatio': 1,\n",
       " 'enamel': 1,\n",
       " 'ajvc': 1,\n",
       " 'straightens': 3,\n",
       " 'infill': 1,\n",
       " 'lovg': 2,\n",
       " 'ddd': 3,\n",
       " 'translucent': 2,\n",
       " 'centrifuge': 3,\n",
       " 'heartily': 9,\n",
       " 'yazz': 1,\n",
       " 'absorbers': 1,\n",
       " 'suglia': 1,\n",
       " 'replies': 125,\n",
       " 'bunuel': 1,\n",
       " 'hould': 1,\n",
       " 'sugar': 35,\n",
       " 'spgs': 1,\n",
       " 'busting': 8,\n",
       " 'kronk': 1,\n",
       " 'ovy': 1,\n",
       " 'catalina': 4,\n",
       " 'actuation': 1,\n",
       " 'convicingly': 1,\n",
       " 'multiplication': 3,\n",
       " 'hamstring': 1,\n",
       " 'constabulary': 1,\n",
       " 'inevitablity': 1,\n",
       " 'albelin': 3,\n",
       " 'maksut': 1,\n",
       " 'scourging': 3,\n",
       " 'penalizing': 2,\n",
       " 'rocicrusian': 1,\n",
       " 'asbestos': 15,\n",
       " 'seirio': 5,\n",
       " 'rjf': 3,\n",
       " 'lesc': 2,\n",
       " 'mellodew': 1,\n",
       " 'bosse': 1,\n",
       " 'transkei': 1,\n",
       " 'healthy': 65,\n",
       " 'tuinstrd': 6,\n",
       " 'rv': 9,\n",
       " 'krew': 1,\n",
       " 'enlighting': 1,\n",
       " 'tolerates': 1,\n",
       " 'pump': 27,\n",
       " 'unlikely': 111,\n",
       " 'porthole': 1,\n",
       " 'increases': 53,\n",
       " 'captivity': 3,\n",
       " 'talanted': 2,\n",
       " 'liquid': 30,\n",
       " 'attracts': 2,\n",
       " 'kepley': 2,\n",
       " 'sjc': 1,\n",
       " 'amendments': 18,\n",
       " 'summarise': 3,\n",
       " 'germinal': 2,\n",
       " 'xtappsettypeconverter': 4,\n",
       " 'writes': 5751,\n",
       " 'mzurmw': 1,\n",
       " 'lyddy': 4,\n",
       " 'ndallen': 10,\n",
       " 'defaultrootwindow': 7,\n",
       " 'formulate': 8,\n",
       " 'puzzled': 15,\n",
       " 'shortstop': 16,\n",
       " 'submitted': 31,\n",
       " 'socalist': 1,\n",
       " 'bit': 793,\n",
       " 'defecate': 2,\n",
       " 'les': 30,\n",
       " 'knowability': 1,\n",
       " 'raider': 1,\n",
       " 'zebedee': 2,\n",
       " 'mcw': 1,\n",
       " 'asthsma': 1,\n",
       " 'dregistere': 1,\n",
       " 'monocrome': 1,\n",
       " 'integrals': 1,\n",
       " 'mustard': 6,\n",
       " 'lngut': 1,\n",
       " 'enthusiasts': 10,\n",
       " 'jarryl': 1,\n",
       " 'spacify': 4,\n",
       " 'wjqj': 1,\n",
       " 'hockey': 233,\n",
       " 'shapewindow': 1,\n",
       " 'noao': 2,\n",
       " 'electrophoresis': 4,\n",
       " 'sq': 19,\n",
       " 'reducible': 3,\n",
       " 'facs': 3,\n",
       " 'reformation': 4,\n",
       " 'palace': 12,\n",
       " 'conically': 1,\n",
       " 'khudurova': 1,\n",
       " 'merton': 3,\n",
       " 'seymo': 1,\n",
       " 'mmw': 2,\n",
       " 'savannah': 1,\n",
       " 'hambidge': 19,\n",
       " 'capus': 2,\n",
       " 'uterine': 1,\n",
       " 'wadddoes': 1,\n",
       " 'segmentation': 9,\n",
       " 'zaandam': 3,\n",
       " 'cristiano': 1,\n",
       " 'guide': 114,\n",
       " 'bkline': 2,\n",
       " 'vlbzrl': 1,\n",
       " 'logistical': 4,\n",
       " 'plase': 3,\n",
       " 'fnajr': 2,\n",
       " 'neath': 2,\n",
       " 'arky': 2,\n",
       " 'biol': 1,\n",
       " 'ukrphil': 1,\n",
       " 'statistisk': 2,\n",
       " 'anasaz': 1,\n",
       " 'afairs': 9,\n",
       " 'proceedeth': 1,\n",
       " 'talionis': 1,\n",
       " 'vulnerabilites': 1,\n",
       " 'siyasi': 2,\n",
       " 'zju': 1,\n",
       " 'minh': 3,\n",
       " 'incorrect': 71,\n",
       " 'hoffa': 2,\n",
       " 'moins': 10,\n",
       " 'oxidises': 1,\n",
       " 'fins': 1,\n",
       " 'cooperstown': 1,\n",
       " 'vacaville': 2,\n",
       " 'arrgggghhhh': 1,\n",
       " 'usian': 1,\n",
       " 'caprette': 1,\n",
       " 'interestend': 1,\n",
       " 'jesper': 3,\n",
       " 'nationalities': 5,\n",
       " 'applies': 97,\n",
       " 'sometimes': 342,\n",
       " 'substantial': 49,\n",
       " 'star': 118,\n",
       " 'sppace': 1,\n",
       " 'cleared': 24,\n",
       " 'pct': 10,\n",
       " 'zur': 3,\n",
       " 'mlap': 1,\n",
       " 'louise': 1,\n",
       " 'rigorous': 18,\n",
       " 'resul': 1,\n",
       " 'decant': 3,\n",
       " 'aud': 2,\n",
       " 'fore': 5,\n",
       " 'urbane': 1,\n",
       " 'perenial': 1,\n",
       " 'corelmark': 4,\n",
       " 'xtgrabnone': 1,\n",
       " 'sigelman': 1,\n",
       " 'tvinfo': 1,\n",
       " 'fokes': 11,\n",
       " 'marquees': 1,\n",
       " 'suggeted': 1,\n",
       " 'bweulv': 1,\n",
       " 'agabus': 1,\n",
       " 'stretch': 38,\n",
       " 'brazenio': 1,\n",
       " 'entertain': 11,\n",
       " 'favorites': 8,\n",
       " 'azzam': 1,\n",
       " 'sethf': 1,\n",
       " 'contorted': 2,\n",
       " 'nobodies': 1,\n",
       " 'notdaw': 1,\n",
       " 'rpms': 6,\n",
       " 'tzxb': 1,\n",
       " 'cccccc': 1,\n",
       " 'eskulap': 1,\n",
       " 'mattix': 1,\n",
       " 'llcoolj': 1,\n",
       " 'grim': 5,\n",
       " 'parroted': 3,\n",
       " 'forteresse': 1,\n",
       " 'kitaguchi': 2,\n",
       " 'juha': 7,\n",
       " 'huh': 92,\n",
       " 'mom': 34,\n",
       " 'arromdee': 29,\n",
       " 'rebuttals': 1,\n",
       " 'qhs': 1,\n",
       " 'checklist': 2,\n",
       " 'inboxes': 2,\n",
       " 'kasler': 1,\n",
       " 'gentile': 20,\n",
       " 'xsizehints': 4,\n",
       " 'pagel': 1,\n",
       " 'stardog': 2,\n",
       " 'bonds': 36,\n",
       " 'repentance': 20,\n",
       " 'desqviewx': 2,\n",
       " 'bushes': 3,\n",
       " 'urbin': 2,\n",
       " 'tend': 155,\n",
       " 'judizers': 1,\n",
       " 'festival': 9,\n",
       " 'recruitment': 6,\n",
       " 'jamma': 1,\n",
       " 'aziz': 5,\n",
       " 'paschich': 1,\n",
       " 'complemented': 1,\n",
       " 'fumbled': 1,\n",
       " 'dobie': 1,\n",
       " 'nead': 4,\n",
       " 'bumping': 3,\n",
       " 'thought': 811,\n",
       " 'censured': 2,\n",
       " 'soennichsen': 1,\n",
       " 'fouling': 1,\n",
       " 'whw': 1,\n",
       " 'disqualification': 1,\n",
       " 'connectivity': 7,\n",
       " 'luna': 6,\n",
       " 'leftout': 1,\n",
       " 'gland': 4,\n",
       " 'wre': 1,\n",
       " 'offerred': 1,\n",
       " 'navigate': 2,\n",
       " 'harass': 7,\n",
       " 'starting': 206,\n",
       " 'consultant': 43,\n",
       " 'republicans': 37,\n",
       " 'amassed': 4,\n",
       " 'hiroaki': 1,\n",
       " 'congressgrades': 1,\n",
       " 'cognac': 1,\n",
       " 'koinoia': 1,\n",
       " 'erzeroum': 1,\n",
       " 'uab': 5,\n",
       " 'objecten': 1,\n",
       " 'mills': 3,\n",
       " 'pfz': 1,\n",
       " 'mcrosbie': 1,\n",
       " 'nervousness': 2,\n",
       " 'xhh': 1,\n",
       " 'dent': 13,\n",
       " 'stiffer': 4,\n",
       " 'kizlarini': 1,\n",
       " 'asap': 11,\n",
       " 'famous': 83,\n",
       " 'satellite': 74,\n",
       " 'initialy': 1,\n",
       " 'measurex': 1,\n",
       " 'krh': 2,\n",
       " 'lprsn': 1,\n",
       " 'primitives': 6,\n",
       " 'pursuasive': 2,\n",
       " 'ref': 36,\n",
       " 'harangues': 1,\n",
       " 'char': 26,\n",
       " 'bang': 33,\n",
       " 'melanie': 1,\n",
       " 'klossner': 5,\n",
       " 'castor': 1,\n",
       " 'ivker': 4,\n",
       " 'anthropomorphise': 1,\n",
       " 'flinstone': 1,\n",
       " 'kjrlkir': 1,\n",
       " 'angola': 5,\n",
       " 'palisades': 2,\n",
       " 'gamers': 2,\n",
       " 'scharfy': 25,\n",
       " 'pwatkins': 1,\n",
       " 'annexes': 1,\n",
       " 'mailinglist': 2,\n",
       " 'sophamore': 1,\n",
       " 'gepeinigt': 1,\n",
       " 'sunt': 1,\n",
       " 'genrad': 1,\n",
       " 'freiberg': 1,\n",
       " 'imbeded': 1,\n",
       " 'dr': 65,\n",
       " 'drye': 1,\n",
       " 'exegetical': 2,\n",
       " 'pkane': 1,\n",
       " 'zelots': 1,\n",
       " 'umanitoba': 4,\n",
       " 'acp': 1,\n",
       " 'celery': 1,\n",
       " 'maltais': 4,\n",
       " 'markey': 1,\n",
       " 'sidewall': 1,\n",
       " 'mactools': 1,\n",
       " 'limelight': 1,\n",
       " 'xbell': 2,\n",
       " 'respiratory': 6,\n",
       " 'wgp': 2,\n",
       " 'olin': 1,\n",
       " 'praktische': 2,\n",
       " 'wurst': 1,\n",
       " 'eccentric': 8,\n",
       " 'verging': 1,\n",
       " 'rescue': 17,\n",
       " 'brynas': 3,\n",
       " 'motorola': 137,\n",
       " 'wght': 1,\n",
       " 'ldh': 1,\n",
       " 'wanton': 2,\n",
       " 'sommerfeld': 2,\n",
       " 'komite': 2,\n",
       " 'thant': 3,\n",
       " 'officialese': 2,\n",
       " 'jw': 13,\n",
       " 'biegel': 1,\n",
       " 'mapped': 25,\n",
       " 'uncertainty': 10,\n",
       " 'cement': 3,\n",
       " 'tunk': 1,\n",
       " 'lah': 4,\n",
       " 'diddleysquatpoop': 1,\n",
       " 'rr': 17,\n",
       " 'logicon': 2,\n",
       " 'eqj': 1,\n",
       " 'hfe': 2,\n",
       " 'rossiya': 1,\n",
       " 'uev': 2,\n",
       " 'republished': 1,\n",
       " 'xtwindowofobject': 6,\n",
       " 'means': 683,\n",
       " 'accompanying': 19,\n",
       " 'soyuz': 6,\n",
       " 'thedialog': 1,\n",
       " 'evokes': 2,\n",
       " 'westboro': 9,\n",
       " 'slater': 2,\n",
       " 'dimple': 1,\n",
       " 'snivelling': 1,\n",
       " 'prutchi': 6,\n",
       " 'forsaking': 4,\n",
       " 'governed': 11,\n",
       " 'buf': 24,\n",
       " 'predictably': 3,\n",
       " 'encryp': 2,\n",
       " 'internationalized': 1,\n",
       " 'fairwell': 1,\n",
       " 'matrices': 4,\n",
       " 'bitblaster': 1,\n",
       " 'banker': 2,\n",
       " 'jxi': 1,\n",
       " 'ogil': 1,\n",
       " 'atul': 2,\n",
       " 'weeny': 1,\n",
       " 'witha': 3,\n",
       " 'bloodcount': 3,\n",
       " 'respectful': 5,\n",
       " 'combinations': 22,\n",
       " 'consensually': 3,\n",
       " 'tvc': 1,\n",
       " 'falcione': 4,\n",
       " 'villanova': 4,\n",
       " 'capriccioso': 9,\n",
       " 'vznl': 1,\n",
       " 'antagonize': 1,\n",
       " 'proposals': 35,\n",
       " 'immaturity': 6,\n",
       " 'vangen': 1,\n",
       " 'maritime': 1,\n",
       " 'diagrams': 18,\n",
       " 'denial': 34,\n",
       " 'whl': 1,\n",
       " 'telescoping': 3,\n",
       " 'allmartin': 2,\n",
       " 'coccidiodes': 1,\n",
       " 'slams': 3,\n",
       " 'bloodstreams': 1,\n",
       " 'action': 279,\n",
       " 'subwoofer': 4,\n",
       " 'mcreynolds': 3,\n",
       " 'nehls': 1,\n",
       " 'kch': 2,\n",
       " 'works': 595,\n",
       " 'apm': 1,\n",
       " 'quietely': 1,\n",
       " 'herman': 9,\n",
       " 'tns': 1,\n",
       " 'cuny': 1,\n",
       " 'tegretol': 1,\n",
       " 'earlobe': 4,\n",
       " 'ozvk': 1,\n",
       " 'propositions': 3,\n",
       " 'vyf': 2,\n",
       " 'maginal': 1,\n",
       " 'understandings': 1,\n",
       " 'snapix': 1,\n",
       " 'remappings': 2,\n",
       " 'contenintal': 1,\n",
       " 'finkelstein': 1,\n",
       " 'excesses': 5,\n",
       " 'murderous': 11,\n",
       " 'bull': 45,\n",
       " 'rlhz': 2,\n",
       " 'fro': 2,\n",
       " 'basa': 2,\n",
       " 'mih': 1,\n",
       " 'consume': 23,\n",
       " 'kinikliouglu': 1,\n",
       " 'hillman': 1,\n",
       " 'truckers': 2,\n",
       " 'inadequate': 20,\n",
       " 'expresses': 14,\n",
       " 'tcc': 1,\n",
       " 'alishaw': 1,\n",
       " 'ifthe': 1,\n",
       " 'hesitation': 2,\n",
       " 'characteristically': 2,\n",
       " 'bram': 3,\n",
       " 'bmw': 126,\n",
       " 'decoding': 15,\n",
       " 'spindle': 1,\n",
       " 'subcontinent': 1,\n",
       " 'sgp': 1,\n",
       " 'entried': 1,\n",
       " 'drv': 1,\n",
       " 'nesim': 1,\n",
       " 'zware': 1,\n",
       " 'mobasseri': 5,\n",
       " 'scandanavians': 1,\n",
       " 'hangs': 36,\n",
       " 'staticcolor': 2,\n",
       " 'wechselbaelge': 1,\n",
       " 'europeenne': 1,\n",
       " 'caruth': 1,\n",
       " 'jaehyung': 2,\n",
       " 'alfred': 9,\n",
       " 'jagani': 3,\n",
       " 'gomorrha': 1,\n",
       " 'ces': 2,\n",
       " 'dfeldman': 2,\n",
       " 'ahaz': 1,\n",
       " 'reserving': 1,\n",
       " 'portruding': 1,\n",
       " 'digit': 20,\n",
       " 'keysyms': 3,\n",
       " 'waldrop': 2,\n",
       " 'ruzun': 1,\n",
       " 'tried': 499,\n",
       " 'eorsat': 1,\n",
       " 'intelligences': 2,\n",
       " 'ermeni': 7,\n",
       " 'supremisists': 1,\n",
       " 'restricting': 22,\n",
       " 'odyssey': 3,\n",
       " 'cryptologists': 2,\n",
       " 'challe': 1,\n",
       " 'specificity': 2,\n",
       " 'vakiflar': 1,\n",
       " 'ingest': 2,\n",
       " 'serveral': 2,\n",
       " 'michaelr': 1,\n",
       " 'crnlthry': 1,\n",
       " 'boadilla': 1,\n",
       " 'ethical': 29,\n",
       " 'fractally': 2,\n",
       " 'fairground': 1,\n",
       " 'sexists': 1,\n",
       " 'ajv': 1,\n",
       " 'gamete': 1,\n",
       " 'chastisement': 3,\n",
       " 'capricornus': 2,\n",
       " 'dof': 3,\n",
       " 'unplug': 11,\n",
       " 'cyc': 1,\n",
       " 'doit': 5,\n",
       " 'ahl': 27,\n",
       " 'intellectually': 15,\n",
       " 'luser': 3,\n",
       " 'resolder': 1,\n",
       " 'decorational': 1,\n",
       " 'veff': 1,\n",
       " 'cfv': 7,\n",
       " 'damage': 132,\n",
       " 'wais': 4,\n",
       " 'fury': 6,\n",
       " 'wnv': 1,\n",
       " 'fizzle': 1,\n",
       " 'bdftools': 1,\n",
       " 'psychoenergetic': 1,\n",
       " 'rogerw': 3,\n",
       " 'educate': 21,\n",
       " 'fulfulling': 1,\n",
       " 'jfare': 5,\n",
       " 'garner': 8,\n",
       " 'stable': 64,\n",
       " 'oneida': 1,\n",
       " 'working': 483,\n",
       " 'calls': 207,\n",
       " 'discusion': 6,\n",
       " 'umkjdj': 1,\n",
       " 'heating': 19,\n",
       " 'fairgrove': 1,\n",
       " 'partway': 1,\n",
       " 'anaphylactic': 4,\n",
       " 'hype': 31,\n",
       " 'jabber': 1,\n",
       " 'yns': 1,\n",
       " 'whitey': 2,\n",
       " 'persay': 1,\n",
       " 'expectedly': 1,\n",
       " 'kuyers': 1,\n",
       " 'massachussets': 1,\n",
       " 'rightness': 2,\n",
       " 'nokes': 6,\n",
       " 'msll': 1,\n",
       " 'cared': 18,\n",
       " 'deskwriter': 5,\n",
       " 'automatic': 108,\n",
       " 'constitutions': 13,\n",
       " 'afternoon': 43,\n",
       " 'btw': 273,\n",
       " 'unsettling': 2,\n",
       " 'dcw': 2,\n",
       " 'pkeenan': 1,\n",
       " 'gentleman': 14,\n",
       " 'inverted': 4,\n",
       " 'mamishev': 2,\n",
       " 'aepc': 1,\n",
       " 'antifungals': 1,\n",
       " 'beetle': 5,\n",
       " 'dovetail': 3,\n",
       " 'elastomers': 1,\n",
       " 'cites': 24,\n",
       " 'mwm': 38,\n",
       " 'myxkg': 1,\n",
       " 'polished': 9,\n",
       " 'turkler': 2,\n",
       " 'torkel': 1,\n",
       " 'wynn': 3,\n",
       " 'composing': 3,\n",
       " 'smartdrive': 9,\n",
       " 'djcoyle': 1,\n",
       " 'pitchout': 1,\n",
       " 'succubi': 1,\n",
       " 'ranheim': 3,\n",
       " 'backo': 1,\n",
       " 'ylnth': 1,\n",
       " 'blockiness': 3,\n",
       " 'poljot': 1,\n",
       " 'formula': 24,\n",
       " 'summarizes': 10,\n",
       " 'sportswriters': 6,\n",
       " 'rifling': 1,\n",
       " 'kg': 25,\n",
       " 'dalcs': 4,\n",
       " 'lsq': 1,\n",
       " 'seefeld': 3,\n",
       " 'importing': 12,\n",
       " 'investigation': 69,\n",
       " 'metaphorically': 4,\n",
       " 'barring': 11,\n",
       " 'turret': 1,\n",
       " 'idol': 3,\n",
       " 'kleriklemek': 1,\n",
       " 'stefano': 1,\n",
       " 'quilty': 3,\n",
       " 'industru': 1,\n",
       " 'reactor': 19,\n",
       " 'vermeille': 1,\n",
       " 'populated': 11,\n",
       " 'speculator': 1,\n",
       " 'racked': 2,\n",
       " 'yub': 1,\n",
       " 'chihuahua': 2,\n",
       " 'negotiable': 17,\n",
       " 'disbanded': 4,\n",
       " 'cedric': 1,\n",
       " 'fixer': 2,\n",
       " 'minerals': 8,\n",
       " 'revenge': 28,\n",
       " 'imperialism': 3,\n",
       " 'handheld': 10,\n",
       " 'hesd': 1,\n",
       " 'sequiter': 1,\n",
       " 'unimpressive': 1,\n",
       " 'timescales': 1,\n",
       " 'relapse': 1,\n",
       " 'mitsubushi': 1,\n",
       " 'consumerism': 1,\n",
       " 'transactional': 1,\n",
       " 'byw': 1,\n",
       " 'beal': 7,\n",
       " 'sniffing': 3,\n",
       " 'pixmap': 37,\n",
       " 'singularly': 1,\n",
       " 'nrp': 5,\n",
       " 'talatinian': 1,\n",
       " 'drinckes': 1,\n",
       " 'mjuric': 1,\n",
       " 'mccullou': 10,\n",
       " 'cq': 5,\n",
       " 'benzocaine': 1,\n",
       " 'forms': 92,\n",
       " 'merkurs': 1,\n",
       " 'feel': 522,\n",
       " 'leafing': 1,\n",
       " 'superiour': 1,\n",
       " 'logging': 10,\n",
       " 'kenyon': 2,\n",
       " 'bigdog': 2,\n",
       " 'vica': 2,\n",
       " 'anvance': 1,\n",
       " 'nasty': 51,\n",
       " 'psychadelic': 1,\n",
       " 'wirklich': 1,\n",
       " 'unusual': 82,\n",
       " 'ulkemizin': 1,\n",
       " 'schenectady': 6,\n",
       " 'flanger': 1,\n",
       " 'reservest': 1,\n",
       " 'xnote': 1,\n",
       " 'mylar': 7,\n",
       " 'spokesperson': 10,\n",
       " 'list': 620,\n",
       " 'cgpp': 1,\n",
       " 'jkjec': 5,\n",
       " 'tendonitis': 2,\n",
       " 'mbytes': 3,\n",
       " 'unscientific': 5,\n",
       " 'modulated': 5,\n",
       " 'socialized': 7,\n",
       " 'fij': 8,\n",
       " 'scandinavia': 5,\n",
       " 'enjoyable': 5,\n",
       " 'silliness': 2,\n",
       " 'pooled': 1,\n",
       " 'techbooks': 2,\n",
       " 'trafic': 2,\n",
       " 'quadrinomials': 1,\n",
       " 'lsbfirst': 1,\n",
       " 'disassociates': 1,\n",
       " 'international': 261,\n",
       " 'mine': 441,\n",
       " 'muesums': 1,\n",
       " 'financiers': 1,\n",
       " 'iir': 1,\n",
       " 'gwu': 1,\n",
       " 'quantum': 63,\n",
       " 'nis': 2,\n",
       " 'tranmit': 2,\n",
       " 'polytheism': 1,\n",
       " 'clarifying': 1,\n",
       " 'lyf': 1,\n",
       " 'avakian': 1,\n",
       " 'pagemaker': 10,\n",
       " 'anyhows': 1,\n",
       " 'alsys': 3,\n",
       " 'exempting': 2,\n",
       " 'arglebargle': 1,\n",
       " 'hamish': 1,\n",
       " 'jbatka': 1,\n",
       " 'hops': 2,\n",
       " 'unreasonably': 5,\n",
       " 'rallied': 1,\n",
       " 'futurenet': 2,\n",
       " 'inlcuded': 2,\n",
       " 'svbgt': 1,\n",
       " 'hydromechanics': 1,\n",
       " 'energized': 1,\n",
       " 'philosophers': 12,\n",
       " 'emg': 3,\n",
       " 'agitating': 1,\n",
       " 'gonorrhoeae': 1,\n",
       " 'schriever': 2,\n",
       " 'wheaties': 1,\n",
       " 'resulted': 41,\n",
       " 'qadian': 1,\n",
       " 'issst': 3,\n",
       " 'zvt': 1,\n",
       " 'instone': 1,\n",
       " 'paralleled': 2,\n",
       " 'pacha': 1,\n",
       " 'standpoint': 18,\n",
       " 'mitubishi': 1,\n",
       " 'evenzo': 1,\n",
       " 'echoes': 5,\n",
       " 'vanderby': 10,\n",
       " 'plugging': 8,\n",
       " 'guykuo': 9,\n",
       " 'teritory': 4,\n",
       " 'superstitions': 1,\n",
       " 'khojali': 2,\n",
       " 'structural': 14,\n",
       " 'gamble': 3,\n",
       " 'bzh': 1,\n",
       " 'nationally': 10,\n",
       " 'seth': 28,\n",
       " 'newsworld': 1,\n",
       " 'digging': 14,\n",
       " 'feeble': 7,\n",
       " 'voa': 1,\n",
       " 'mathematically': 12,\n",
       " 'expensively': 1,\n",
       " 'eih': 1,\n",
       " 'vbkj': 1,\n",
       " 'chalkstone': 1,\n",
       " 'scuba': 1,\n",
       " 'carburator': 1,\n",
       " 'mensch': 1,\n",
       " 'hri': 2,\n",
       " 'cowell': 1,\n",
       " 'ekdfc': 3,\n",
       " 'acting': 58,\n",
       " 'trivializers': 1,\n",
       " 'leono': 1,\n",
       " 'theophilus': 1,\n",
       " 'zarmooni': 1,\n",
       " 'reunions': 2,\n",
       " 'dimmer': 5,\n",
       " 'amantadine': 1,\n",
       " 'finchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchmfinchm': 1,\n",
       " 'fingertip': 1,\n",
       " 'gruss': 2,\n",
       " 'muflers': 1,\n",
       " 'tons': 36,\n",
       " 'altitude': 30,\n",
       " 'immunocompromised': 4,\n",
       " 'infmx': 3,\n",
       " 'dms': 6,\n",
       " 'jlpicard': 1,\n",
       " 'transaxles': 1,\n",
       " 'quits': 2,\n",
       " 'munday': 1,\n",
       " 'physican': 2,\n",
       " 'salamen': 1,\n",
       " 'neils': 1,\n",
       " 'idct': 1,\n",
       " 'ibrahimoglu': 1,\n",
       " 'cased': 1,\n",
       " 'unjustified': 5,\n",
       " 'carte': 1,\n",
       " 'sevr': 1,\n",
       " 'blass': 1,\n",
       " 'meer': 2,\n",
       " 'yhvh': 1,\n",
       " 'sprung': 3,\n",
       " 'knbc': 1,\n",
       " 'grf': 6,\n",
       " 'ecclesiological': 1,\n",
       " 'subtracted': 2,\n",
       " 'theologica': 4,\n",
       " 'nuggets': 4,\n",
       " 'lightsails': 1,\n",
       " 'cracker': 8,\n",
       " 'segard': 6,\n",
       " 'hauling': 1,\n",
       " 'hypothesis': 34,\n",
       " 'isrmc': 2,\n",
       " 'nui': 3,\n",
       " 'popped': 14,\n",
       " 'northfield': 1,\n",
       " 'gregp': 1,\n",
       " 'gulls': 3,\n",
       " 'eacj': 1,\n",
       " 'ecosystems': 1,\n",
       " 'mlk': 11,\n",
       " 'edt': 44,\n",
       " 'cents': 24,\n",
       " 'ctrl': 11,\n",
       " 'fences': 4,\n",
       " 'obviousness': 1,\n",
       " 'ys': 8,\n",
       " 'bacalzo': 1,\n",
       " 'xakellis': 1,\n",
       " 'hilger': 1,\n",
       " 'bain': 6,\n",
       " 'satie': 1,\n",
       " 'euax': 1,\n",
       " 'pcarmack': 2,\n",
       " 'salerno': 1,\n",
       " 'fastmicro': 4,\n",
       " 'isas': 4,\n",
       " 'kosher': 12,\n",
       " 'victories': 10,\n",
       " 'creationist': 8,\n",
       " ...}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary containing the frequency of words utilizing the 'frequency_dict' function\n",
    "\n",
    "# Expect this chunk to take a comparatively longer time to execute since our dataset is large\n",
    "\n",
    "freq_word = frequency_dict(lines)\n",
    "\n",
    "freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "vLvPijR_GKHn"
   },
   "outputs": [],
   "source": [
    "# Create a function to calculate the Term Frequency\n",
    "\n",
    "def term_frequency(document, word):\n",
    "    '''\n",
    "    document: list containing the entire corpus\n",
    "    word: word whose term frequency is to be calculated\n",
    "    ---\n",
    "    tf: returns term frequency value\n",
    "    '''\n",
    "#     count = 0\n",
    "#     for token in document:\n",
    "#         if token == word:\n",
    "#             count = count + 1\n",
    "    \n",
    "#     tf = count/len(document)\n",
    "            \n",
    "#     return tf\n",
    "\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "HA99G_yAGLCC"
   },
   "outputs": [],
   "source": [
    "# Create a function to calculate the Inverse Document Frequency\n",
    " \n",
    "def inverse_df(word):\n",
    "    '''\n",
    "    word: word whose inverse document frequency is to be calculated\n",
    "    ---\n",
    "    idf: return inverse document frequency value\n",
    "    '''\n",
    "\n",
    "#     idf = math.log(total_docs / freq_word[word])\n",
    "#     return idf\n",
    "\n",
    "    try:\n",
    "        word_occurance = freq_word[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_documents/word_occurance + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "F0irgwv2GRfE"
   },
   "outputs": [],
   "source": [
    "#Create a function to combine the term frequencies (TF) and inverse document (IDF) frequencies calculated above to get TF-IDF\n",
    "\n",
    "def tfidf(sentence,dict_idx):\n",
    "    '''\n",
    "    sentence: list containing the entire corpus\n",
    "    dict: dictionary keeping track of index of each word\n",
    "    ---\n",
    "    tf_idf_vec: returns computed tf-idf\n",
    "    '''\n",
    "\n",
    "#     for word in dict_idx:        \n",
    "#         tf_idf_vec = term_frequency(sentence, word) * inverse_df(word)      \n",
    "#     return tf_idf_vec\n",
    "\n",
    "    tf_idf_vec = np.zeros((len(word_list),))\n",
    "    for word in sentence:\n",
    "        tf = term_frequency(sentence,word)\n",
    "        idf = inverse_df(word)       \n",
    "        value = tf*idf\n",
    "        tf_idf_vec[dict_idx[word]] = value \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "_VKJhqatGWpV"
   },
   "outputs": [],
   "source": [
    "#Compute the vectors utilizing the 'tfidf' function created above to obtain a TF-IDF Encoded text corpus\n",
    "tf_idf_df = []\n",
    "#print(lines)\n",
    "for each_text in lines:\n",
    "#    print(each_text)\n",
    "    tf_idf_df.append (tfidf(each_text,dict_idx))\n",
    "    \n",
    "tf_idf_df = pd.DataFrame(tf_idf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE0UGUaSGb8I"
   },
   "source": [
    "## Multinomial Naive Bayes (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "yWYcxrdJGfDC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1      2      3      4      5      6      7      8      9      \\\n",
      "0        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "11309    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11310    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11311    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11312    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11313    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "       ...  73233  73234  73235  73236  73237  73238  73239  73240  73241  \\\n",
      "0      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "11309  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11310  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11311  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11312  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "11313  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "       73242  \n",
      "0        0.0  \n",
      "1        0.0  \n",
      "2        0.0  \n",
      "3        0.0  \n",
      "4        0.0  \n",
      "...      ...  \n",
      "11309    0.0  \n",
      "11310    0.0  \n",
      "11311    0.0  \n",
      "11312    0.0  \n",
      "11313    0.0  \n",
      "\n",
      "[11314 rows x 73243 columns]\n"
     ]
    }
   ],
   "source": [
    "#Fit a Multinomial Naive Bayes Model on our dataset\n",
    "#np_array = np.array(tf_idf_vec)\n",
    "print(tf_idf_df)\n",
    "model = MultinomialNB().fit(tf_idf_df, mydata_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "G6CiQB4qGfqH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 4 ... 3 1 8]\n"
     ]
    }
   ],
   "source": [
    "#Perform testing on the train dataset\n",
    "\n",
    "pred = model.predict(tf_idf_df)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "yCLagGu6Gh6T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.8883683931412409\n",
      "Accuracy:  0.8883683931412409\n"
     ]
    }
   ],
   "source": [
    "#Calculate the F1 Score and the Accuracy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "F1_score = f1_score(mydata_train.target, pred, average='micro')\n",
    "Accuracy = metrics.accuracy_score(mydata_train.target, pred)\n",
    "print(\"F1 Score: \", F1_score)\n",
    "print(\"Accuracy: \", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbMRqJv5Gl2F"
   },
   "source": [
    "### Expected Output:\n",
    "F1 Score: 0.9533633964397735\n",
    "\n",
    "Accuracy: 0.9524482941488421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWRDuUqU-taV"
   },
   "source": [
    "Your accuracy does not have to be exactly the same. This is just to give you an estimate of what could you expect your accuracy to be around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfMc8cz93Cc0"
   },
   "source": [
    "## Question 2 Vector Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70iwEeL23F7K"
   },
   "source": [
    "In this unsupervised learning task we are going to cluster wikipedia articles into groups using T-SNE visualization after vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHx4YuxW36oM"
   },
   "source": [
    "### Collect articles from Wikipedia (10 points)\n",
    "\n",
    "In this section we will download articles from wikipedia and then vectorize them in the next step. You can select somewhat related topics or fetch the articles randomly. \n",
    "(Use dir() and help() functions or refer wikipedia documentation)\n",
    "You may also pick any other data source of your choice instead of wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "jA419x6__mjg"
   },
   "outputs": [],
   "source": [
    "# install libraries\n",
    "#pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vLMLk4K84Zbn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "Yale University\n",
      "Harvard University\n",
      "Princeton University\n",
      "Massachusetts Institute of Technology\n",
      "Washington University in St. Louis\n",
      "Boston University\n",
      "Seattle\n",
      "California\n",
      "Silicon Valley\n",
      "Chicago\n",
      "Texas\n",
      "Natural language processing\n",
      "Machine learning\n",
      "Quantum machine learning\n",
      "Artificial intelligence\n",
      "Data science\n",
      "Master in Data Science\n",
      "Analytics\n",
      "Data mining\n",
      "Information retrieval\n",
      "Robotics\n",
      "Database\n",
      "Visa Inc.\n",
      "European Central Bank\n",
      "Financial technology\n",
      "International Monetary Fund\n",
      "Jeffrey Dean Morgan\n",
      "Swimming\n",
      "Tennis\n",
      "Football\n",
      "Association football\n",
      "Cricket\n",
      "Badminton\n",
      "Kabaddi\n",
      "Table tennis\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    " Generate a list of wikipedia article to cluster \n",
    " You can maintain a static list of titles or generate them randomly using wikipedia library\n",
    " Some topics include:\n",
    " [\"Northeastern Unversity\", \"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \"Data science\", \"Master in Data Science\", \n",
    " \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Bank\", \"Financial technology\",\"International Monetary Fund\", \n",
    " \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"College Football\", \"Association Football\"]\n",
    "\n",
    " You can add more topics from different categories so that we have a diverse dataset to work with. \n",
    " Ex- About 3+ categories(groups), 3+ topics in each category, 3+ articles in each topic\n",
    "'''\n",
    "\n",
    "# selected topics\n",
    "topics = [\"Yale University\", \"Harvard\", \"Princeton\", \"MIT\", \"Washington University\",\"University of Texas\"\n",
    "          \"Boston\", \"Seattle\", \"California\", \"Silicon Valley\", \"Chicago\",\"Texas\",\n",
    "          \"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \n",
    "          \"Data science\", \"Master in Data Science\",\"Data Analytics\", \"Data Mining\", \"Information Retrieval\",\"Robotics\",\"Database Management System\"\n",
    "          \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Financial technology\",\"International Monetary Fund\",\"J.P. Morgan\" \n",
    "          \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"Association Football\",\"Cricket\",\"Badminton\", \"Kabaddi\", \"Table Tennis\"]\n",
    "\n",
    "print(len(topics))\n",
    "# list of articles to be downloaded\n",
    "articles = []\n",
    "for topic in topics:\n",
    "    \n",
    "    articles.append(wikipedia.search(topic))\n",
    "# download and store articles (summaries) in this variable  \n",
    "data = []\n",
    "for article in articles:  \n",
    "        print(article[0])\n",
    "#         internal_list = []\n",
    "#         internal_list.append(article[0])\n",
    "        summary = wikipedia.summary(article)\n",
    "#         internal_list.append(summary)\n",
    "        data.append(summary)\n",
    "print(len(articles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgpRv7wQ4Dpm"
   },
   "source": [
    "### Cleaning the Data (5 points)\n",
    "In this step you will decide whether to clean the data or not. If you choose to clean, you may utilize the clean function from assignment 1.\n",
    "\n",
    "**Question:** Why are you (not) choosing to clean the data? Think in terms of whether cleaning data will help in the clustering or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnZpDKcaHTGq"
   },
   "source": [
    "**Answer(1-3 sentences):** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lNj53Pxr963N"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SnowballStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#nltk.download('punkt')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#nltk.download('stopwords')\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m snowball \u001b[38;5;241m=\u001b[39m \u001b[43mSnowballStemmer\u001b[49m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_message\u001b[39m(message):\n\u001b[1;32m     10\u001b[0m     no_html \u001b[38;5;241m=\u001b[39m BeautifulSoup(message)\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SnowballStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "# You can use Assignment 1's clean message function\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "snowball = SnowballStemmer(language='english')\n",
    "def clean_message(message):\n",
    "    no_html = BeautifulSoup(message).get_text()\n",
    "#     clean = re.sub(\"[^a-z\\s]+\", \" \", no_html, flags=re.IGNORECASE)\n",
    " #   clean = re.sub(\"(\\s+)\", \" \", clean)\n",
    "    clean = re.sub(\"(\\s+)\", \" \", no_html)\n",
    "    clean = clean.lower()\n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "    cleaned_stopwords = [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]\n",
    "    stemmed_words = []\n",
    "    for w in cleaned_stopwords:\n",
    "        stemmed_words.append(snowball.stem(w))\n",
    "    return stemmed_words\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvRZUpmq-DmT"
   },
   "source": [
    "### Vectorize the articles (5 points)\n",
    "\n",
    "In this step, we will vectorize the text data. You can use TfidfVectorizer() or countVectorizer() from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gJk8YY89-OU4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.17556307 0.         ... 0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['000', '10', '100', ..., '', '', ''],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(X.toarray())\n",
    "vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DAIGlqEuINWA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1916)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKLvrKHRQaQq"
   },
   "source": [
    "### Sample Output:\n",
    "(36, 1552)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5ZrGrzD_G8d"
   },
   "source": [
    "### Plot Articles (10 points)\n",
    "Now we will try to verify the groups of articles using T-SNE from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SjcuZBOe-oZq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neo/Library/Python/3.8/lib/python/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/neo/Library/Python/3.8/lib/python/site-packages/sklearn/manifold/_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  32.277195   169.3485   ]\n",
      " [ 128.88712     -1.7707925]\n",
      " [  70.61778     74.99576  ]\n",
      " [  -7.093404   -89.24093  ]\n",
      " [ -15.082539   134.83875  ]\n",
      " [ -36.36747    183.78378  ]\n",
      " [ -11.396402   -32.997272 ]\n",
      " [  28.114832    45.735996 ]\n",
      " [ 110.03617    -85.79566  ]\n",
      " [  37.887283   108.13808  ]\n",
      " [ -45.86455     -5.052034 ]\n",
      " [ 125.04152     64.7591   ]\n",
      " [ -95.843704   -35.050205 ]\n",
      " [  52.35847   -112.89976  ]\n",
      " [ -56.72784    -60.496994 ]\n",
      " [-127.95796    -84.7303   ]\n",
      " [  33.73234   -167.02634  ]\n",
      " [-113.492744  -143.40201  ]\n",
      " [ -78.7108    -101.48956  ]\n",
      " [  39.645893   -61.855766 ]\n",
      " [ 115.65418    125.86101  ]\n",
      " [ -34.689434  -149.20393  ]\n",
      " [-104.283745    20.842813 ]\n",
      " [  32.50862     -8.676596 ]\n",
      " [-161.60951    -16.307098 ]\n",
      " [ 167.3477     -51.97895  ]\n",
      " [ -85.24537    133.10478  ]\n",
      " [ -13.202006    80.83767  ]\n",
      " [ 190.07867     52.917347 ]\n",
      " [  84.23612    -30.965973 ]\n",
      " [  73.21128     18.022736 ]\n",
      " [  -4.399315    16.254837 ]\n",
      " [ -40.969933    44.13151  ]\n",
      " [ -75.07698     73.98809  ]\n",
      " [-145.18774     79.2507   ]]\n",
      "          Dim1        Dim2  labels  Categories\n",
      "0    32.277195  169.348495     0.0           0\n",
      "1   128.887115   -1.770792     1.0           0\n",
      "2    70.617783   74.995758     2.0           0\n",
      "3    -7.093404  -89.240929     3.0           0\n",
      "4   -15.082539  134.838745     4.0           0\n",
      "5   -36.367470  183.783783     5.0           0\n",
      "6   -11.396402  -32.997272     6.0           1\n",
      "7    28.114832   45.735996     7.0           1\n",
      "8   110.036171  -85.795662     8.0           1\n",
      "9    37.887283  108.138077     9.0           1\n",
      "10  -45.864552   -5.052034    10.0           1\n",
      "11  125.041519   64.759102    11.0           2\n",
      "12  -95.843704  -35.050205    12.0           2\n",
      "13   52.358471 -112.899757    13.0           2\n",
      "14  -56.727840  -60.496994    14.0           2\n",
      "15 -127.957962  -84.730301    15.0           2\n",
      "16   33.732342 -167.026337    16.0           2\n",
      "17 -113.492744 -143.402008    17.0           2\n",
      "18  -78.710800 -101.489563    18.0           2\n",
      "19   39.645893  -61.855766    19.0           2\n",
      "20  115.654182  125.861008    20.0           2\n",
      "21  -34.689434 -149.203934    21.0           2\n",
      "22 -104.283745   20.842813    22.0           3\n",
      "23   32.508621   -8.676596    23.0           3\n",
      "24 -161.609512  -16.307098    24.0           3\n",
      "25  167.347702  -51.978951    25.0           3\n",
      "26  -85.245369  133.104782    26.0           3\n",
      "27  -13.202006   80.837669    27.0           4\n",
      "28  190.078674   52.917347    28.0           4\n",
      "29   84.236122  -30.965973    29.0           4\n",
      "30   73.211281   18.022736    30.0           4\n",
      "31   -4.399315   16.254837    31.0           4\n",
      "32  -40.969933   44.131512    32.0           4\n",
      "33  -75.076981   73.988091    33.0           4\n",
      "34 -145.187744   79.250702    34.0           4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.manifold import TSNE\n",
    "    \n",
    "\n",
    "# call TSNE() to fit the data\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "labels = []\n",
    "index = 0\n",
    "for val in topics:\n",
    "    labels.append(index)\n",
    "    index = index + 1\n",
    "    \n",
    "markers = [] \n",
    "mark = 0\n",
    "for val in groups:\n",
    "    markers.append(mark)\n",
    "    mark = mark + 1\n",
    "    \n",
    "X_embedded_stack = np.vstack((X_embedded.T, labels )).T\n",
    "X_embedded_df = pd.DataFrame(data = X_embedded_stack, columns = (\"Dim1\", \"Dim2\",\"labels\" ))\n",
    "X_embedded_df[\"Categories\"] = [0, 0, 0, 0,0,0,\n",
    "                              1, 1, 1, 1, 1,\n",
    "                              2, 2, 2, 2, 2, 2,\n",
    "                              2, 2, 2, 2, 2, \n",
    "                              3, 3, 3, 3, 3,\n",
    "                              4, 4, 4, 4, 4, 4, 4, 4]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCY_blxjO1bs"
   },
   "source": [
    "Plot and annotate the points with different markers for different expected groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 169.3485   ,   -1.7707925,   74.99576  ,  -89.24093  ,\n",
       "        134.83875  ,  183.78378  ,  -32.997272 ,   45.735996 ,\n",
       "        -85.79566  ,  108.13808  ,   -5.052034 ,   64.7591   ,\n",
       "        -35.050205 , -112.89976  ,  -60.496994 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded[:,1][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "3ODUA1Vf-rRd",
    "outputId": "325b5db4-60d1-4907-a487-40893e256ee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Vector Visualization')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import matplotlib.patheffects as PathEffects\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "colors = X_embedded_df[\"Categories\"]\n",
    "num_classes = len(np.unique(colors))\n",
    "labels = X_embedded_df[\"labels\"]\n",
    "palette = np.array(sn.color_palette(\"hls\", num_classes))\n",
    "markers =  np.array([\"d\", \"v\", \"s\", \"*\", \"^\"])    \n",
    "f = plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(aspect='equal')\n",
    "sc = ax.scatter(X_embedded[:,0], X_embedded[:,1])\n",
    "plt.xlim(-25, 25)\n",
    "plt.ylim(-25, 25)\n",
    "ax.axis('tight')\n",
    "\n",
    "present_category = 0\n",
    "categories =  X_embedded_df[\"Categories\"].values\n",
    "for idx in range(len(categories)):\n",
    "    category = categories[idx]\n",
    "    if(category == present_category):\n",
    "        plt.plot(X_embedded[idx][0], X_embedded[idx][1],color = palette[category], marker = markers[category], markersize = 10)\n",
    "    else:\n",
    "        present_category = present_category + 1\n",
    "        plt.plot(X_embedded[idx][0], X_embedded[idx][1],color = palette[category], marker = markers[category], markersize = 10)\n",
    "        \n",
    "for i in range(len(labels)):       \n",
    "    plt.annotate(labels[i], (X_embedded[i][0], X_embedded[i][1] + 0.3))\n",
    "\n",
    "\n",
    "ax.set_title('Vector Visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYcEi1EC_UGO"
   },
   "source": [
    "**Question:** Comment about the categorizion done by T-SNE. Do the articles of related topics cluster together? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tde1Wu5HI6AA"
   },
   "source": [
    "**Answer(1-3 sentences):**  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2xrJddIpSsd"
   },
   "source": [
    "# Question 3 Building Neural Networks\n",
    "\n",
    "### We are gonna use Emotions Dataset for this task. We need to classify the given text into different kind of emotions like happy,sad,anger etc.., \n",
    "\n",
    "### We are providing train.txt and val.txt files along with this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJYmVbkvphgX"
   },
   "source": [
    "### Library Imports and Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BO-HU-7uorVX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "#string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "  # From the last assignment\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www.\\S+\", \"\", text)\n",
    "    text_links_removed = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text_cleaned = \" \".join([word for word in re.split('\\W+', text_links_removed)\n",
    "        if word not in stopword])\n",
    "    text = \" \".join([wn.lemmatize(word) for word in re.split('\\W+', text_cleaned)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0D5wVjZw7s0"
   },
   "source": [
    "### Q) Importing the datasets and do the necessary cleaning and convert the text into the vectors which are mentioned in the below code blocks. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MOMhmIlGprK9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import the train.txt and val.txt file into pandas dataframe format \n",
    "\n",
    "## Reading the data and removing columns that are not important. \n",
    "\n",
    "# train \n",
    "train = pd.read_csv(\"train.txt\", sep=\";\", header=None, names=['text', 'emotion'])\n",
    "\n",
    "# validation\n",
    "validation = pd.read_csv(\"val.txt\", sep=\";\" , header=None, names=['text', 'emotion'])\n",
    "# and printout the train.shape and validation.shape \n",
    "print(train.shape)\n",
    "print(validation.shape)\n",
    "\n",
    "# self addition\n",
    "# labels_dict = {'sadness':0, 'joy':1, 'love':2, 'anger':3, 'fear':4, 'surprise':5}\n",
    "# train['label'] = train['emotion'].map(labels_dict )\n",
    "# train.head()\n",
    "# expected shape of train dataset is (16000,2) and validation dataset is (2000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PS7K3p7AqAI8"
   },
   "outputs": [],
   "source": [
    "# clean the text in the train and validation dataframes using the clean_text function provided above\n",
    "\n",
    "text = train['text']\n",
    "cleaned_text_train = []\n",
    "# print(train.iloc[0]['text'])\n",
    "for sentence in text:\n",
    "    sentence = clean_text(sentence)\n",
    "    cleaned_text_train.append(sentence)\n",
    "train['text'] = cleaned_text_train  \n",
    "\n",
    "cleaned_text_val = []\n",
    "\n",
    "validation_text = validation['text']\n",
    "for sentence in validation_text:\n",
    "    sentence = clean_text(sentence)\n",
    "    cleaned_text_val.append(sentence)\n",
    "validation['text'] = cleaned_text_val      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GxZC6RIjq3bu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 13457)\n",
      "(16000, 13457)\n",
      "(16000, 7)\n"
     ]
    }
   ],
   "source": [
    "# initialise count vectorizer from sklearn module with default parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit on train dataset and transform both train and validation dataset\n",
    "X = train['text'].values\n",
    "vectorizer.fit(X)\n",
    "X_train_count_vector =  vectorizer.transform(X)\n",
    "print(X_train_count_vector.shape)\n",
    "count_train_df = pd.DataFrame(X_train_count_vector.toarray())\n",
    "print(count_train_df.shape)\n",
    "print(train.shape)\n",
    "val = validation['text'].values\n",
    "X_val_count_vector = vectorizer.fit_transform(val).toarray()\n",
    "\n",
    "# y = train['emotion'].values\n",
    "# train_text, train_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1000)\n",
    "# train_text_fit = vectorizer.fit(train_text)\n",
    "\n",
    "# X_train_count_vector = vectorizer.transform(train_text_fit)\n",
    "\n",
    "\n",
    "# X = validation['text'].values\n",
    "# y = validation['emotion'].values\n",
    "# val_text, val_test, y_train, y_test = train_test_split(X, y, test_size=None, random_state=1000)\n",
    "# val_text_fit = vectorizer.fit(val_text)\n",
    "# X_val_count_vector = vectorizer.transform(val_text_fit)\n",
    "\n",
    "\n",
    "# X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "# val_text = vectorizer.fit(\n",
    "\n",
    "# X_train_count_vector =  vectorizer.transform(train_text)\n",
    "# text'].values)\n",
    "# X_validation_count_vector = vectorizer.transform(val_text)\n",
    "#X_validation_count_vector_df = np.asarray(X_validation_count_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "85nuazEzq3YT"
   },
   "outputs": [],
   "source": [
    "# initialise tfidf vectorizer from sklearn module with default parameter\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# fit on train dataset and transform both train and validation dataset\n",
    "X_train_tfidf_vector = vectorizer.fit_transform(train['text'])\n",
    "X_validation_tfidf_vector = vectorizer.fit_transform(validation['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jgHWAuG-q3Vm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                              didnt feel humiliated        4\n",
       "1  go feeling hopeless damned hopeful around some...        4\n",
       "2          im grabbing minute post feel greedy wrong        0\n",
       "3  ever feeling nostalgic fireplace know still pr...        3\n",
       "4                                    feeling grouchy        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise label encoder from sklearn module\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "# fit on train labels and transform both train and validation labels\n",
    "train['emotion'] = le.fit_transform(train['emotion'])\n",
    "\n",
    "validation['emotion'] = le.fit(validation['emotion'])\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Wjdye0tvq3So"
   },
   "outputs": [],
   "source": [
    "# convert the labels into one hot encoding form\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehotencoder = OneHotEncoder(sparse = False, handle_unknown = 'error', drop = 'first')\n",
    "onehotencoder_train_df = pd.DataFrame(onehotencoder.fit_transform(train[['emotion']]))\n",
    "train = train.join(onehotencoder_train_df)\n",
    "onehotencoder_val_df = pd.DataFrame(onehotencoder.fit_transform(validation[['emotion']]))\n",
    "# one_hot_encoded_data_train = pd.get_dummies(train, columns = ['emotion'])\n",
    "\n",
    "# one_hot_encoded_data_val = pd.get_dummies(validation, columns = ['emotion'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjsiH8YOw-Da"
   },
   "source": [
    "### Q) Build the neural networks using tensorflow keras by following the below instructions. Evaluate the model on different metrics and comment your observations. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AQg14bkTq3KB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# complete this linear model in tensorflow\n",
    "def build_model(X):\n",
    "\n",
    "  # layer 1 : input layer\n",
    "  inp = tf.keras.Input((X.shape[0],))\n",
    "  print(inp.shape) \n",
    "  \n",
    "  # layer 2 : add the dense layer with 2048 units and relu activation\n",
    "  x = tf.keras.layers.Dense(2048,input_dim = X.shape[0], activation = \"relu\")(inp)\n",
    "  print(x.shape) \n",
    "  # layer 3 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)  \n",
    "  print(x.shape) \n",
    "  # layer 4 : add the dense layer with 1024 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(1024, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape) \n",
    "  # layer 5 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)  \n",
    "  print(x.shape) \n",
    "  # layer 6 : add the dense layer with 512 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(512, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape)   \n",
    "  # layer 7 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x) \n",
    "  print(x.shape) \n",
    "  # layer 8 : add the dense layer with 256 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(256, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape) \n",
    "  # layer 9 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)\n",
    "  print(x.shape) \n",
    "  # layer 10 : add the dense layer with 128 units with tanh activation and with l2 regularization\n",
    "  x = tf.keras.layers.Dense(128, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "  print(x.shape)   \n",
    "  # layer 11 : add the dropout layer with dropout rate of 0.5\n",
    "  x = tf.keras.layers.Dropout(.5)(x)\n",
    "  print(x.shape) \n",
    "  # layer 12 : output layer with units equal to number of classes and activation as softmax\n",
    "  x = tf.keras.layers.Dense(5,activation = 'softmax')(x) \n",
    "  print(x.shape) \n",
    "    \n",
    "  model = tf.keras.models.Model(inputs=inp, outputs=x)\n",
    "  print(inp.shape)\n",
    "  print(x.shape) \n",
    "  opt = tf.keras.optimizers.Adam(learning_rate=1e-03)\n",
    "  # use loss as categorical crossentropy, optimizer as rmsprop and evaluate model on auc,precision,recall,accuracy \n",
    "  model.compile(loss = 'sparse_categorical_crossentropy' , optimizer = opt, metrics = ['sparse_categorical_accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Q71CC1pIsx0O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16000)\n",
      "(None, 2048)\n",
      "(None, 2048)\n",
      "(None, 1024)\n",
      "(None, 1024)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 128)\n",
      "(None, 128)\n",
      "(None, 5)\n",
      "(None, 16000)\n",
      "(None, 5)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 16000)]           0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 2048)              32770048  \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,557,893\n",
      "Trainable params: 35,557,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# call the build_model function and initialize the model\n",
    "model = build_model(X_train_count_vector)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KyAZNgsBsxwo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "16000\n",
      "2000\n",
      "2000\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Python/3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Python/3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 16000), found shape=(8, 13457)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_val_count_vector\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# X_train_count_vector = X_train_count_vector.toarray()\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_count_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_count_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_count_vector)\n",
      "File \u001b[0;32m/Library/Python/3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/7t/h643y62j38j54bfnf6lt27rm0000gn/T/__autograph_generated_fileerjzvcwf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Python/3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Python/3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Python/3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 16000), found shape=(8, 13457)\n"
     ]
    }
   ],
   "source": [
    "# train and validate the model on the count vectors of text which we have created initially for 10 epochs, \n",
    "# adjust batch size according to your computation power (suggestion use : 8)\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='tutorial.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy',factor=0.1,patience=3)\n",
    "print(train['emotion'].shape[0])\n",
    "print(count_train_df.shape[0])\n",
    "print(validation['emotion'].shape[0])\n",
    "print(X_val_count_vector.shape[0])\n",
    "# X_train_count_vector = X_train_count_vector.toarray()\n",
    "history = model.fit(X_train_count_vector, train['emotion'], epochs=10, batch_size = 8,validation_data=(X_val_count_vector, validation['emotion']),callbacks=[model_checkpoint_callback,lr_schedule] ) \n",
    "y_pred = model.predict(X_train_count_vector)\n",
    "#print(f'The Overall accuracy score is {accuracy_score(X_train_count_vector, y_pred)}')\n",
    "\n",
    "#loss and optimizer\n",
    "# loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# optim = keras.optimizers.Adam(lr=0.001)\n",
    "# metrics = [\"accuracy\"]\n",
    "\n",
    "# model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "\n",
    "# # training\n",
    "# batch_size = 64\n",
    "# epochs = 5\n",
    "\n",
    "# model.fit(X_train_count_vector, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, verbose=2)\n",
    "\n",
    "# # evaulate\n",
    "# model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "nS02IwLCsxmG"
   },
   "outputs": [],
   "source": [
    "# plot train loss vs val loss, train auc vs val auc, train recall vs val recall, train precision vs val precision and train accuracy vs val accuracy and comment your observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "6FHdCcp7wXyw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16000)\n",
      "(None, 2048)\n",
      "(None, 2048)\n",
      "(None, 1024)\n",
      "(None, 1024)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 128)\n",
      "(None, 128)\n",
      "(None, 5)\n",
      "(None, 16000)\n",
      "(None, 5)\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 16000)]           0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 2048)              32770048  \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,557,893\n",
      "Trainable params: 35,557,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# again call the build_model function and initialize the model\n",
    "model = build_model(train['text'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4gB80M6wXvV"
   },
   "outputs": [],
   "source": [
    "# train and validate the model on the tfidf vectors of text which we have created initially for 10 epochs, \n",
    "# adjust batch size according to your computation power (suggestion use : 8)\n",
    "\n",
    "model = build_model(X_train_tfidf_vector)\n",
    "y_pred = model.predict(X_train_tfidf_vector)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'The Overall accuracy score is {accuracy_score(X_train_tfidf_vector['emotion'], y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsnEOHXRwXkv"
   },
   "outputs": [],
   "source": [
    "# plot train loss vs val loss, train auc vs val auc, train recall vs val recall, train precision vs val precision and train accuracy vs val accuracy and comment your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4adDDQWeQyGb"
   },
   "source": [
    "## Question 4 Theory Question  \n",
    "\n",
    "What is the difference between Count Vectorizer, TFIDF, Word2Vec and Glove? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXTRkF6KRB_D"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "Glove and Word2vec are both unsupervised models for generating word vectors. The difference between them is the mechanism of generating word vectors. The word vectors generated by either of these models can be used for a wide variety of tasks ranging such as\n",
    "\n",
    "    finding words that are semantically similar to a word,\n",
    "    representing a word when it is being input to a downstream model. A word embedding representation of a word captures more information about a word than just a one-hot representation of the word, since the former captures semantic similarity of that word to other words whereas the latter representation of the word is equidistant from all other words.\n",
    "\n",
    "Tf-idf is a scoring scheme for words - that is a measure of how important a word is to a document.\n",
    "\n",
    "From a practical usage standpoint, while tf-idf is a simple scoring scheme and that is its key advantage, word embeddings may be a better choice for most tasks where tf-idf is used, particularly when the task can benefit from the semantic similarity captured by word embeddings (e.g. in information retrieval tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQQsqcto79zE"
   },
   "source": [
    "What is the significant difference between the Niave Bayes Implementation using Bag of Words and TF-IDF? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtiPCTZM8Aua"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaXBo1Bw8C1K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS6120_NLP_Assignment_2_Notebook",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
